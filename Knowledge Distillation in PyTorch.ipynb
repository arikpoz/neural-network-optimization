{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaA5o4SVlYp4"
   },
   "source": [
    "# Knowledge Distillation in PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmEt_Jt_mN3H"
   },
   "source": [
    "## Basic Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "v9pAalyPmPNP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.8.0.dev20250319+cu128\n",
      "Device used: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device used: {device.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB-mvTJSllp5"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ovNSIvy1lMgy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 45000\n",
      "Validation samples: 5000\n",
      "Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# # Use 224Ã—224 resize for ResNet compatibility\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "# transform = models.ResNet50_Weights.IMAGENET1K_V2.transforms()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],  # CIFAR-10 means\n",
    "                         std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "# Load full CIFAR-10 train set\n",
    "full_trainset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Calculate split sizes for train and validation sets\n",
    "train_size = int(0.9 * len(full_trainset))\n",
    "val_size = len(full_trainset) - train_size\n",
    "\n",
    "# Perform split\n",
    "train_subset, val_subset = random_split(full_trainset, [train_size, val_size])\n",
    "print(f\"Train samples: {train_size}\")\n",
    "print(f\"Validation samples: {val_size}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=128, shuffle=False)\n",
    "\n",
    "# CIFAR-10 test set and loader for accuracy evaluation\n",
    "test_set = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False)\n",
    "print(f\"Test samples: {len(test_set)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-g0mwY2lloz6"
   },
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P1d3oOmrlpoc",
    "outputId": "7ff4fca2-ec01-42f2-b53d-9a59a2fc9691"
   },
   "outputs": [],
   "source": [
    "# Teacher: ResNet50 pretrained on ImageNet, re-headed for CIFAR-10\n",
    "teacher = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "teacher.fc = nn.Linear(2048, 10)\n",
    "\n",
    "# Student: ResNet18 from scratch\n",
    "student = models.resnet18(weights=None)\n",
    "student.fc = nn.Linear(512, 10)\n",
    "\n",
    "teacher = teacher.to(device)\n",
    "student = student.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, loader, epochs, tag, lr=1e-3, save_path=\"model.pth\", silent=False):\n",
    "    \"\"\"\n",
    "    Trains a model with Adam and cross-entropy loss.\n",
    "    Loads from save_path if it exists.\n",
    "    \"\"\"\n",
    "        \n",
    "    if os.path.exists(save_path):\n",
    "        if not silent:\n",
    "            print(f\"Model already trained. Loading from {save_path}\")\n",
    "        model.load_state_dict(torch.load(save_path))\n",
    "        return\n",
    "\n",
    "    # no saved model found. training from given model state\n",
    "\n",
    "    optimizer = torch.optim.Adam(teacher.parameters(), lr=1e-3)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = teacher(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if not silent:\n",
    "            print(f\"({tag})\\tEpoch {epoch+1}: loss={loss.item():.4f}\")\n",
    "\n",
    "            evaluate_accuracy(model, val_loader, f\"Epoch {epoch+1}\")\n",
    "            model.train()\n",
    "            \n",
    "    if save_path:\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        if not silent:\n",
    "            print(f\"Training complete. Model saved to {save_path}\")\n",
    "\n",
    "# Function to evaluate accuracy given loader\n",
    "def evaluate_accuracy(model, dataloader, tag):\n",
    "    \"\"\"\n",
    "    Evaluate and print accuracy given loader\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy ({tag}): {accuracy*100:.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssnZ6fOplu0i"
   },
   "source": [
    "## Fine-Tune the Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5g9mWr4jlxFR",
    "outputId": "40896b4a-65f0-4a35-c837-7a0aa0d5da0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already trained. Loading from resnet50_ImageNet1K_pretrained_CIFAR10_tuned.pth\n"
     ]
    }
   ],
   "source": [
    "# Train the teacher on CIFAR-10 for a few epochs\n",
    "train(teacher, train_loader, epochs=10, tag=\"Fine-tuning teacher\", save_path=\"resnet50_ImageNet1K_pretrained_CIFAR10_tuned.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUnxajeIPDSS"
   },
   "source": [
    "## Intermediate Feature Extraction for Matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "SpFwD7-pPDsb"
   },
   "outputs": [],
   "source": [
    "# Extract final logits and selected intermediate outputs\n",
    "def extract_features(model, x, layers=[1, 2, 3, 4]):\n",
    "    features = []\n",
    "    x = model.conv1(x)\n",
    "    x = model.bn1(x)\n",
    "    x = model.relu(x)\n",
    "    x = model.maxpool(x)\n",
    "    for i, block in enumerate([model.layer1, model.layer2, model.layer3, model.layer4]):\n",
    "        x = block(x)\n",
    "        if (i + 1) in layers:\n",
    "            features.append(x)\n",
    "    return x, features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate Distillation with Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureProjector(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, target_shape):\n",
    "        # Resize spatial dims if needed (e.g., from 8x8 to 4x4)\n",
    "        if x.shape[2:] != target_shape[2:]:\n",
    "            x = F.adaptive_avg_pool2d(x, output_size=target_shape[2:])\n",
    "        return self.proj(x)\n",
    "\n",
    "student_channels = [64, 128, 256, 512]\n",
    "teacher_channels = [256, 512, 1024, 2048]\n",
    "\n",
    "proj_layers = nn.ModuleList([\n",
    "    FeatureProjector(in_c, out_c).to(device)\n",
    "    for in_c, out_c in zip(student_channels, teacher_channels)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYQ-AOinlrET"
   },
   "source": [
    "## Distillation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "K6Qv15vVltWi"
   },
   "outputs": [],
   "source": [
    "# Combine soft and hard targets using KL divergence and cross-entropy\n",
    "# T = temperature, alpha = weighting between soft and hard losses\n",
    "def distillation_loss(student_logits, teacher_logits, targets, T=5.0, alpha=0.7):\n",
    "    # Soft target loss (teacher softmax vs student softmax)\n",
    "    soft_targets = F.kl_div(\n",
    "        F.log_softmax(student_logits / T, dim=1),\n",
    "        F.softmax(teacher_logits / T, dim=1),\n",
    "        reduction='batchmean'\n",
    "    ) * (T * T)\n",
    "    # Hard label loss\n",
    "    hard_loss = F.cross_entropy(student_logits, targets)\n",
    "    return alpha * soft_targets + (1 - alpha) * hard_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtpKJq_H4jJW"
   },
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "J_snm7CV4j1x"
   },
   "outputs": [],
   "source": [
    "# Function to count trainable parameters\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Function to measure average inference latency over multiple runs\n",
    "def measure_latency(model, input_size=(1, 3, 32, 32), device='cuda', repetitions=50):\n",
    "    model.eval()\n",
    "    inputs = torch.randn(input_size).to(device)\n",
    "    with torch.no_grad():\n",
    "        # Warm-up\n",
    "        for _ in range(10):\n",
    "            _ = model(inputs)\n",
    "        # Measure\n",
    "        times = []\n",
    "        for _ in range(repetitions):\n",
    "            start = time.time()\n",
    "            _ = model(inputs)\n",
    "            end = time.time()\n",
    "            times.append(end - start)\n",
    "    return (sum(times) / repetitions) * 1000  # ms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wiv5X7eVlydg"
   },
   "source": [
    "## Train the Student via Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "W-DTZmLXPgK9",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2048) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m     feat_loss += F.mse_loss(s_proj, t_feat.detach())\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Soft + hard loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m loss = \u001b[43mdistillation_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m + \u001b[32m0.1\u001b[39m * feat_loss\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# # Feature matching loss (intermediate distillation)\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# feat_loss = sum(F.mse_loss(s, t.detach()) for s, t in zip(student_feats, teacher_feats))\u001b[39;00m\n\u001b[32m     39\u001b[39m \n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# loss = distillation_loss(student_logits, teacher_logits, labels) + 0.1 * feat_loss\u001b[39;00m\n\u001b[32m     42\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mdistillation_loss\u001b[39m\u001b[34m(student_logits, teacher_logits, targets, T, alpha)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdistillation_loss\u001b[39m(student_logits, teacher_logits, targets, T=\u001b[32m5.0\u001b[39m, alpha=\u001b[32m0.7\u001b[39m):\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Soft target loss (teacher softmax vs student softmax)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     soft_targets = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkl_div\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent_logits\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher_logits\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatchmean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m * (T * T)\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Hard label loss\u001b[39;00m\n\u001b[32m     11\u001b[39m     hard_loss = F.cross_entropy(student_logits, targets)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3396\u001b[39m, in \u001b[36mkl_div\u001b[39m\u001b[34m(input, target, size_average, reduce, reduction, log_target)\u001b[39m\n\u001b[32m   3393\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3394\u001b[39m         reduction_enum = _Reduction.get_enum(reduction)\n\u001b[32m-> \u001b[39m\u001b[32m3396\u001b[39m reduced = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkl_div\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3398\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reduction == \u001b[33m\"\u001b[39m\u001b[33mbatchmean\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28minput\u001b[39m.dim() != \u001b[32m0\u001b[39m:\n\u001b[32m   3399\u001b[39m     reduced = reduced / \u001b[38;5;28minput\u001b[39m.size()[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (2048) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Train the student using the teacher's output as soft targets\n",
    "teacher.eval()  # Teacher in eval mode\n",
    "optimizer = torch.optim.Adam(student.parameters(), lr=1e-3)\n",
    "\n",
    "# Reduce LR if validation loss doesn't improve for 3 epochs\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "for epoch in range(50):  # Adjust as needed\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Teacher outputs (frozen)\n",
    "        with torch.no_grad():\n",
    "            teacher_logits, teacher_feats = extract_features(teacher, inputs)\n",
    "\n",
    "        # Student outputs\n",
    "        student_logits, student_feats = extract_features(student, inputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # Project and match all intermediate layers\n",
    "        feat_loss = 0\n",
    "        for s_feat, t_feat, proj in zip(student_feats, teacher_feats, proj_layers):\n",
    "            s_proj = proj(s_feat, t_feat.shape)\n",
    "            feat_loss += F.mse_loss(s_proj, t_feat.detach())\n",
    "        \n",
    "        # Soft + hard loss\n",
    "        loss = distillation_loss(student_logits, teacher_logits, labels) + 0.1 * feat_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # # Feature matching loss (intermediate distillation)\n",
    "        # feat_loss = sum(F.mse_loss(s, t.detach()) for s, t in zip(student_feats, teacher_feats))\n",
    "\n",
    "        # loss = distillation_loss(student_logits, teacher_logits, labels) + 0.1 * feat_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    val_accuracy = evaluate_accuracy(student, val_loader, device)\n",
    "    print(f\"(Training student)\\tEpoch {epoch+1}: Loss = {loss.item():.4f}, Val Accuracy={val_accuracy*100:.2f}%\")\n",
    "    scheduler.step(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_J2dwzIl1ss"
   },
   "source": [
    "## Model Comparison Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ldidj0f_l4P0",
    "outputId": "adf38296-4433-4c80-f377-f976d41dbfa8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Compare size, latency, and accuracy\n",
    "teacher_params = count_params(teacher)\n",
    "student_params = count_params(student)\n",
    "\n",
    "teacher_latency = measure_latency(teacher, device=device)\n",
    "student_latency = measure_latency(student, device=device)\n",
    "\n",
    "teacher_acc = evaluate_accuracy(teacher, test_loader, device)\n",
    "student_acc = evaluate_accuracy(student, test_loader, device)\n",
    "\n",
    "print(f\"Teacher Params: {teacher_params / 1e6:.2f}M\")\n",
    "print(f\"Student Params: {student_params / 1e6:.2f}M\")\n",
    "print(f\"Teacher Latency: {teacher_latency:.2f} ms\")\n",
    "print(f\"Student Latency: {student_latency:.2f} ms\")\n",
    "print(f\"Teacher Test Accuracy: {teacher_acc * 100:.2f}%\")\n",
    "print(f\"Student Test Accuracy: {student_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYg6HZDivYK1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
