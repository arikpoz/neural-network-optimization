{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaA5o4SVlYp4"
      },
      "source": [
        "# Knowledge Distillation in PyTorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmEt_Jt_mN3H"
      },
      "source": [
        "## Basic Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v9pAalyPmPNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35bb8d4d-636a-46f5-ddbe-97bef9975061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.6.0+cu124\n",
            "Device used: cuda\n",
            "Debug mode: False\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device used: {device.type}\")\n",
        "debug = False\n",
        "print(f\"Debug mode: {debug}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB-mvTJSllp5"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ovNSIvy1lMgy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d3253d0-f7c2-45e1-9997-9654ef190479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 45000\n",
            "Validation samples: 5000\n",
            "Test samples: 10000\n"
          ]
        }
      ],
      "source": [
        "# # Use 224×224 resize for ResNet compatibility\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),\n",
        "#     transforms.ToTensor()\n",
        "# ])\n",
        "# transform = models.ResNet50_Weights.IMAGENET1K_V2.transforms()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],  # CIFAR-10 means\n",
        "                         std=[0.2023, 0.1994, 0.2010])\n",
        "])\n",
        "\n",
        "# Load full CIFAR-10 train set\n",
        "full_trainset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Calculate split sizes for train and validation sets\n",
        "train_size = int(0.9 * len(full_trainset))\n",
        "val_size = len(full_trainset) - train_size\n",
        "\n",
        "# Perform split\n",
        "train_subset, val_subset = random_split(full_trainset, [train_size, val_size])\n",
        "print(f\"Train samples: {train_size}\")\n",
        "print(f\"Validation samples: {val_size}\")\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=128, shuffle=False)\n",
        "\n",
        "# CIFAR-10 test set and loader for accuracy evaluation\n",
        "test_set = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_set, batch_size=128, shuffle=False)\n",
        "print(f\"Test samples: {len(test_set)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g0mwY2lloz6"
      },
      "source": [
        "## Define Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup teacher and student wrapper\n",
        "def setup_models(device):\n",
        "    # Teacher: ResNet50 pretrained on ImageNet, re-headed for CIFAR-10\n",
        "    teacher = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "    teacher.fc = nn.Linear(2048, 10)\n",
        "    teacher = teacher.to(device)\n",
        "\n",
        "    # Student: ResNet18 from scratch\n",
        "    student = models.resnet18(weights=None)\n",
        "    student.fc = nn.Linear(512, 10).to(device)\n",
        "    student = student.to(device)\n",
        "\n",
        "    student_channels = [64, 128, 256, 512]\n",
        "    teacher_channels = [256, 512, 1024, 2048]\n",
        "    proj_layers = [\n",
        "        FeatureProjector(in_c, out_c).to(device)\n",
        "        for in_c, out_c in zip(student_channels, teacher_channels)\n",
        "    ]\n",
        "\n",
        "    student_wrapper = StudentWrapper(student, proj_layers).to(device)\n",
        "    return teacher, student_wrapper"
      ],
      "metadata": {
        "id": "7X5j6K6rBZEX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapping the Student Model with Projections\n",
        "\n",
        "When using intermediate supervision (feature matching), the student’s feature maps often don't match the teacher’s in shape. To fix this, we wrap the student model and its feature projection layers into a single StudentWrapper module:"
      ],
      "metadata": {
        "id": "kzWqbPGB30I0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zkYfMKlfE3R2"
      },
      "outputs": [],
      "source": [
        "# Feature projector to match student -> teacher feature shapes\n",
        "class FeatureProjector(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, target_shape):\n",
        "        if x.shape[2:] != target_shape[2:]:\n",
        "            x = F.adaptive_avg_pool2d(x, output_size=target_shape[2:])\n",
        "        return self.proj(x)\n",
        "\n",
        "# Wrapper for the student model with projection layers\n",
        "class StudentWrapper(nn.Module):\n",
        "    def __init__(self, student_model, proj_layers):\n",
        "        super().__init__()\n",
        "        self.model = student_model\n",
        "        self.projections = nn.ModuleList(proj_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        x = self.model.conv1(x)\n",
        "        x = self.model.bn1(x)\n",
        "        x = self.model.relu(x)\n",
        "        x = self.model.maxpool(x)\n",
        "        for i, block in enumerate([self.model.layer1, self.model.layer2, self.model.layer3, self.model.layer4]):\n",
        "            x = block(x)\n",
        "            features.append(x)\n",
        "        pooled = F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        flat = torch.flatten(pooled, 1)\n",
        "        logits = self.model.fc(flat)\n",
        "        return logits, features\n",
        "\n",
        "    def project_features(self, features, target_shapes):\n",
        "        return [\n",
        "            proj(s_feat, t_shape)\n",
        "            for s_feat, t_shape, proj in zip(features, target_shapes, self.projections)\n",
        "        ]\n",
        "\n",
        "# Extract teacher logits and intermediate features\n",
        "def extract_features(model, x, layers=[1, 2, 3, 4]):\n",
        "    features = []\n",
        "    x = model.conv1(x)\n",
        "    x = model.bn1(x)\n",
        "    x = model.relu(x)\n",
        "    x = model.maxpool(x)\n",
        "    for i, block in enumerate([model.layer1, model.layer2, model.layer3, model.layer4]):\n",
        "        x = block(x)\n",
        "        if (i + 1) in layers:\n",
        "            features.append(x)\n",
        "\n",
        "    # Final classification head\n",
        "    pooled = F.adaptive_avg_pool2d(x, (1, 1))  # [B, C, 1, 1]\n",
        "    flat = torch.flatten(pooled, 1)            # [B, C]\n",
        "    logits = model.fc(flat)                    # [B, 10]\n",
        "    return logits, features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup models and optimizer"
      ],
      "metadata": {
        "id": "Sak4Av3rCoTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teacher, student_wrapper = setup_models(device)\n",
        "optimizer = torch.optim.Adam(student_wrapper.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "b_u5QCFACmED"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx1Fk_sKE3R2"
      },
      "source": [
        "## Define Train Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Hl60aa5NE3R2"
      },
      "outputs": [],
      "source": [
        "def train_teacher(teacher, loader, epochs, tag, lr=1e-3, save_path=\"model.pth\", silent=False):\n",
        "    \"\"\"\n",
        "    Trains a model with Adam and cross-entropy loss.\n",
        "    Loads from save_path if it exists.\n",
        "    \"\"\"\n",
        "\n",
        "    if os.path.exists(save_path) and debug:\n",
        "        if not silent:\n",
        "            print(f\"Model already trained. Loading from {save_path}\")\n",
        "        teacher.load_state_dict(torch.load(save_path))\n",
        "        return teacher\n",
        "\n",
        "    # no saved model found. training from given model state\n",
        "\n",
        "    optimizer = torch.optim.Adam(teacher.parameters(), lr=1e-3)\n",
        "    teacher.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            logits, _ = extract_features(teacher, inputs)\n",
        "\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if not silent:\n",
        "            print(f\"({tag})\\tEpoch {epoch+1}: loss={loss.item():.4f}\")\n",
        "            # print(f\"[Teacher] Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "            evaluate_accuracy(teacher, val_loader, f\"Epoch {epoch+1}\")\n",
        "            teacher.train()\n",
        "\n",
        "    if save_path:\n",
        "        torch.save(teacher.state_dict(), save_path)\n",
        "        if not silent:\n",
        "            print(f\"Training complete. Model saved to {save_path}\")\n",
        "            print(\"Saved fine-tuned teacher.\")\n",
        "\n",
        "    return teacher\n",
        "\n",
        "# Function to evaluate accuracy given loader\n",
        "def evaluate_accuracy(model, dataloader, tag):\n",
        "    \"\"\"\n",
        "    Evaluate and print accuracy given loader\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    accuracy = correct / total\n",
        "    print(f\"Accuracy ({tag}): {accuracy*100:.2f}%\")\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssnZ6fOplu0i"
      },
      "source": [
        "## Fine-Tune the Teacher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g9mWr4jlxFR",
        "outputId": "2e5e9389-edd6-43a0-95c8-6868009d42eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Fine-tuning teacher)\tEpoch 1: loss=0.7992\n"
          ]
        }
      ],
      "source": [
        "# Train the teacher on CIFAR-10 for a few epochs\n",
        "epochs = 20\n",
        "teacher = train_teacher(teacher, train_loader, epochs=epochs, tag=\"Fine-tuning teacher\", save_path=\"tuned_pretrained_resnet50_on_CIFAR10.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoaZbok4E3R2"
      },
      "source": [
        "## Intermediate Distillation with Projections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYQ-AOinlrET"
      },
      "source": [
        "## Distillation Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6Qv15vVltWi"
      },
      "outputs": [],
      "source": [
        "# Combine soft and hard targets using KL divergence and cross-entropy\n",
        "# T = temperature, alpha = weighting between soft and hard losses\n",
        "def distillation_loss(student_logits, teacher_logits, targets, T=5.0, alpha=0.7):\n",
        "\n",
        "    # Soft target loss (teacher softmax vs student softmax)\n",
        "    soft_targets = F.kl_div(\n",
        "        F.log_softmax(student_logits / T, dim=1),\n",
        "        F.softmax(teacher_logits / T, dim=1),\n",
        "        reduction='batchmean'\n",
        "    ) * (T * T)\n",
        "\n",
        "    # Hard label loss\n",
        "    hard_loss = F.cross_entropy(student_logits, targets)\n",
        "    return alpha * soft_targets + (1 - alpha) * hard_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtpKJq_H4jJW"
      },
      "source": [
        "## Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_snm7CV4j1x"
      },
      "outputs": [],
      "source": [
        "# Function to count trainable parameters\n",
        "def count_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Function to measure average inference latency over multiple runs\n",
        "def measure_latency(model, input_size=(1, 3, 32, 32), device='cuda', repetitions=50):\n",
        "    model.eval()\n",
        "    inputs = torch.randn(input_size).to(device)\n",
        "    with torch.no_grad():\n",
        "        # Warm-up\n",
        "        for _ in range(10):\n",
        "            _ = model(inputs)\n",
        "        # Measure\n",
        "        times = []\n",
        "        for _ in range(repetitions):\n",
        "            start = time.time()\n",
        "            _ = model(inputs)\n",
        "            end = time.time()\n",
        "            times.append(end - start)\n",
        "    return (sum(times) / repetitions) * 1000  # ms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wiv5X7eVlydg"
      },
      "source": [
        "## Train the Student via Distillation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(inputs, labels, teacher, student_wrapper, optimizer, device):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        teacher_logits, teacher_feats = extract_features(teacher, inputs)\n",
        "\n",
        "    student_logits, student_feats = student_wrapper(inputs)\n",
        "    projected_feats = student_wrapper.project_features(student_feats, [t.shape for t in teacher_feats])\n",
        "    feat_loss = sum(F.mse_loss(p, t.detach()) for p, t in zip(projected_feats, teacher_feats))\n",
        "\n",
        "    loss = distillation_loss(student_logits, teacher_logits, labels) + 0.1 * feat_loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "8GAXjD4AB49J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-DTZmLXPgK9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Train the student using the teacher's output as soft targets\n",
        "teacher.eval()  # Teacher in eval mode\n",
        "\n",
        "if debug:\n",
        "  num_epochs = 1\n",
        "else:\n",
        "  num_epochs = 50\n",
        "\n",
        "best_val_acc = 0.0\n",
        "save_path = \"student_distilled.pth\"\n",
        "\n",
        "# Reduce LR if validation loss doesn't improve for 3 epochs\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    student_wrapper.train()\n",
        "    running_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        loss = training_step(inputs, labels, teacher, student_wrapper, optimizer, device)\n",
        "        running_loss += loss\n",
        "\n",
        "    val_acc = evaluate_accuracy(student_wrapper.model, val_loader, \"student\")\n",
        "    print(f\"[(Training student)\\tEpoch {epoch+1}] Loss = {running_loss/len(train_loader):.4f} | Val Acc = {val_acc*100:.2f}%\")\n",
        "    scheduler.step(loss)\n",
        "\n",
        "    # Save best\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(student_wrapper.state_dict(), save_path)\n",
        "        print(\"✅ New best model saved.\")\n",
        "\n",
        "# load best checkpoint\n",
        "student_wrapper.load_state_dict(torch.load(save_path))\n",
        "student = student_wrapper.model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_J2dwzIl1ss"
      },
      "source": [
        "## Model Comparison Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ldidj0f_l4P0"
      },
      "outputs": [],
      "source": [
        "# Compare size, latency, and accuracy\n",
        "teacher_params = count_params(teacher)\n",
        "student_params = count_params(student)\n",
        "\n",
        "teacher_latency = measure_latency(teacher, device=device)\n",
        "student_latency = measure_latency(student, device=device)\n",
        "\n",
        "teacher_acc = evaluate_accuracy(teacher, test_loader, \"teacher on testset\")\n",
        "student_acc = evaluate_accuracy(student, test_loader, \"student on testset\")\n",
        "\n",
        "print(f\"Teacher Params: {teacher_params / 1e6:.2f}M\")\n",
        "print(f\"Student Params: {student_params / 1e6:.2f}M\")\n",
        "print(f\"Teacher Latency: {teacher_latency:.2f} ms\")\n",
        "print(f\"Student Latency: {student_latency:.2f} ms\")\n",
        "print(f\"Teacher Test Accuracy: {teacher_acc * 100:.2f}%\")\n",
        "print(f\"Student Test Accuracy: {student_acc * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYg6HZDivYK1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}