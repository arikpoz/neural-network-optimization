{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaA5o4SVlYp4"
   },
   "source": [
    "# Knowledge Distillation in PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmEt_Jt_mN3H"
   },
   "source": [
    "## Basic Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v9pAalyPmPNP",
    "outputId": "35bb8d4d-636a-46f5-ddbe-97bef9975061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.8.0.dev20250319+cu128\n",
      "Device used: cuda\n",
      "Debug mode: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device used: {device.type}\")\n",
    "debug = False\n",
    "print(f\"Debug mode: {debug}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB-mvTJSllp5"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovNSIvy1lMgy",
    "outputId": "0d3253d0-f7c2-45e1-9997-9654ef190479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 45000\n",
      "Validation samples: 5000\n",
      "Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# # Use 224×224 resize for ResNet compatibility\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "# transform = models.ResNet50_Weights.IMAGENET1K_V2.transforms()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],  # CIFAR-10 means\n",
    "                         std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "# Load full CIFAR-10 train set\n",
    "full_trainset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Calculate split sizes for train and validation sets\n",
    "train_size = int(0.9 * len(full_trainset))\n",
    "val_size = len(full_trainset) - train_size\n",
    "\n",
    "# Perform split\n",
    "train_subset, val_subset = random_split(full_trainset, [train_size, val_size])\n",
    "print(f\"Train samples: {train_size}\")\n",
    "print(f\"Validation samples: {val_size}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=128, shuffle=False)\n",
    "\n",
    "# CIFAR-10 test set and loader for accuracy evaluation\n",
    "test_set = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False)\n",
    "print(f\"Test samples: {len(test_set)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-g0mwY2lloz6"
   },
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7X5j6K6rBZEX"
   },
   "outputs": [],
   "source": [
    "# Setup teacher and student wrapper\n",
    "def setup_models(device):\n",
    "    # Teacher: ResNet50 pretrained on ImageNet, re-headed for CIFAR-10\n",
    "    teacher = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    teacher.fc = nn.Linear(2048, 10)\n",
    "    teacher = teacher.to(device)\n",
    "\n",
    "    # Student: ResNet18 from scratch\n",
    "    student = models.resnet18(weights=None)\n",
    "    student.fc = nn.Linear(512, 10).to(device)\n",
    "    student = student.to(device)\n",
    "\n",
    "    student_channels = [64, 128, 256, 512]\n",
    "    teacher_channels = [256, 512, 1024, 2048]\n",
    "    proj_layers = [\n",
    "        FeatureProjector(in_c, out_c).to(device)\n",
    "        for in_c, out_c in zip(student_channels, teacher_channels)\n",
    "    ]\n",
    "\n",
    "    student_wrapper = StudentWrapper(student, proj_layers).to(device)\n",
    "    return teacher, student_wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzWqbPGB30I0"
   },
   "source": [
    "## Wrapping the Student Model with Projections\n",
    "\n",
    "When using intermediate supervision (feature matching), the student’s feature maps often don't match the teacher’s in shape. To fix this, we wrap the student model and its feature projection layers into a single StudentWrapper module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zkYfMKlfE3R2"
   },
   "outputs": [],
   "source": [
    "# Feature projector to match student -> teacher feature shapes\n",
    "class FeatureProjector(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, target_shape):\n",
    "        if x.shape[2:] != target_shape[2:]:\n",
    "            x = F.adaptive_avg_pool2d(x, output_size=target_shape[2:])\n",
    "        return self.proj(x)\n",
    "\n",
    "# Wrapper for the student model with projection layers\n",
    "class StudentWrapper(nn.Module):\n",
    "    def __init__(self, student_model, proj_layers):\n",
    "        super().__init__()\n",
    "        self.model = student_model\n",
    "        self.projections = nn.ModuleList(proj_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        for i, block in enumerate([self.model.layer1, self.model.layer2, self.model.layer3, self.model.layer4]):\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "        pooled = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        flat = torch.flatten(pooled, 1)\n",
    "        logits = self.model.fc(flat)\n",
    "        return logits, features\n",
    "\n",
    "    def project_features(self, features, target_shapes):\n",
    "        return [\n",
    "            proj(s_feat, t_shape)\n",
    "            for s_feat, t_shape, proj in zip(features, target_shapes, self.projections)\n",
    "        ]\n",
    "\n",
    "# Extract teacher logits and intermediate features\n",
    "def extract_features(model, x, layers=[1, 2, 3, 4]):\n",
    "    features = []\n",
    "    x = model.conv1(x)\n",
    "    x = model.bn1(x)\n",
    "    x = model.relu(x)\n",
    "    x = model.maxpool(x)\n",
    "    for i, block in enumerate([model.layer1, model.layer2, model.layer3, model.layer4]):\n",
    "        x = block(x)\n",
    "        if (i + 1) in layers:\n",
    "            features.append(x)\n",
    "\n",
    "    # Final classification head\n",
    "    pooled = F.adaptive_avg_pool2d(x, (1, 1))  # [B, C, 1, 1]\n",
    "    flat = torch.flatten(pooled, 1)            # [B, C]\n",
    "    logits = model.fc(flat)                    # [B, 10]\n",
    "    return logits, features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sak4Av3rCoTg"
   },
   "source": [
    "## Setup models and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "b_u5QCFACmED"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "teacher, student_wrapper = setup_models(device)\n",
    "optimizer = torch.optim.Adam(student_wrapper.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dx1Fk_sKE3R2"
   },
   "source": [
    "## Define Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Hl60aa5NE3R2"
   },
   "outputs": [],
   "source": [
    "def train_teacher(teacher, loader, epochs, tag, lr=1e-3, save_path=\"model.pth\", silent=False):\n",
    "    \"\"\"\n",
    "    Trains a model with Adam and cross-entropy loss.\n",
    "    Loads from save_path if it exists.\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(save_path) and debug:\n",
    "        if not silent:\n",
    "            print(f\"Model already trained. Loading from {save_path}\")\n",
    "        teacher.load_state_dict(torch.load(save_path))\n",
    "        return teacher\n",
    "\n",
    "    # no saved model found. training from given model state\n",
    "\n",
    "    optimizer = torch.optim.Adam(teacher.parameters(), lr=1e-3)\n",
    "    teacher.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            logits, _ = extract_features(teacher, inputs)\n",
    "\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if not silent:\n",
    "            print(f\"({tag})\\tEpoch {epoch+1}: loss={loss.item():.4f}\")\n",
    "            # print(f\"[Teacher] Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "            evaluate_accuracy(teacher, val_loader, f\"Epoch {epoch+1}\")\n",
    "            teacher.train()\n",
    "\n",
    "    if save_path:\n",
    "        torch.save(teacher.state_dict(), save_path)\n",
    "        if not silent:\n",
    "            print(f\"Training complete. Model saved to {save_path}\")\n",
    "            print(\"Saved fine-tuned teacher.\")\n",
    "\n",
    "    return teacher\n",
    "\n",
    "# Function to evaluate accuracy given loader\n",
    "def evaluate_accuracy(model, dataloader, tag):\n",
    "    \"\"\"\n",
    "    Evaluate and print accuracy given loader\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy ({tag}): {accuracy*100:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssnZ6fOplu0i"
   },
   "source": [
    "## Fine-Tune the Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5g9mWr4jlxFR",
    "outputId": "2e5e9389-edd6-43a0-95c8-6868009d42eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Fine-tuning teacher)\tEpoch 1: loss=0.4818\n",
      "Accuracy (Epoch 1): 81.60%\n",
      "(Fine-tuning teacher)\tEpoch 2: loss=0.6269\n",
      "Accuracy (Epoch 2): 81.70%\n",
      "(Fine-tuning teacher)\tEpoch 3: loss=0.2588\n",
      "Accuracy (Epoch 3): 82.72%\n",
      "(Fine-tuning teacher)\tEpoch 4: loss=0.2500\n",
      "Accuracy (Epoch 4): 82.88%\n",
      "(Fine-tuning teacher)\tEpoch 5: loss=0.1956\n",
      "Accuracy (Epoch 5): 83.52%\n",
      "(Fine-tuning teacher)\tEpoch 6: loss=0.2880\n",
      "Accuracy (Epoch 6): 84.22%\n",
      "(Fine-tuning teacher)\tEpoch 7: loss=0.1356\n",
      "Accuracy (Epoch 7): 84.82%\n",
      "(Fine-tuning teacher)\tEpoch 8: loss=0.2171\n",
      "Accuracy (Epoch 8): 84.50%\n",
      "(Fine-tuning teacher)\tEpoch 9: loss=0.0864\n",
      "Accuracy (Epoch 9): 83.72%\n",
      "(Fine-tuning teacher)\tEpoch 10: loss=0.1164\n",
      "Accuracy (Epoch 10): 84.64%\n",
      "(Fine-tuning teacher)\tEpoch 11: loss=0.1246\n",
      "Accuracy (Epoch 11): 83.88%\n",
      "(Fine-tuning teacher)\tEpoch 12: loss=0.0730\n",
      "Accuracy (Epoch 12): 84.68%\n",
      "(Fine-tuning teacher)\tEpoch 13: loss=0.0834\n",
      "Accuracy (Epoch 13): 85.04%\n",
      "(Fine-tuning teacher)\tEpoch 14: loss=0.2676\n",
      "Accuracy (Epoch 14): 83.54%\n",
      "(Fine-tuning teacher)\tEpoch 15: loss=0.1521\n",
      "Accuracy (Epoch 15): 84.60%\n",
      "(Fine-tuning teacher)\tEpoch 16: loss=0.1297\n",
      "Accuracy (Epoch 16): 85.06%\n",
      "(Fine-tuning teacher)\tEpoch 17: loss=0.0937\n",
      "Accuracy (Epoch 17): 83.96%\n",
      "(Fine-tuning teacher)\tEpoch 18: loss=0.0921\n",
      "Accuracy (Epoch 18): 83.40%\n",
      "(Fine-tuning teacher)\tEpoch 19: loss=0.1168\n",
      "Accuracy (Epoch 19): 84.82%\n",
      "(Fine-tuning teacher)\tEpoch 20: loss=0.0483\n",
      "Accuracy (Epoch 20): 84.14%\n",
      "Training complete. Model saved to tuned_pretrained_resnet50_on_CIFAR10.pth\n",
      "Saved fine-tuned teacher.\n"
     ]
    }
   ],
   "source": [
    "# Train the teacher on CIFAR-10 for a few epochs\n",
    "epochs = 20\n",
    "teacher = train_teacher(teacher, train_loader, epochs=epochs, tag=\"Fine-tuning teacher\", save_path=\"tuned_pretrained_resnet50_on_CIFAR10.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoaZbok4E3R2"
   },
   "source": [
    "## Intermediate Distillation with Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYQ-AOinlrET"
   },
   "source": [
    "## Distillation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "K6Qv15vVltWi"
   },
   "outputs": [],
   "source": [
    "# Combine soft and hard targets using KL divergence and cross-entropy\n",
    "# T = temperature, alpha = weighting between soft and hard losses\n",
    "def distillation_loss(student_logits, teacher_logits, targets, T=5.0, alpha=0.7):\n",
    "\n",
    "    # Soft target loss (teacher softmax vs student softmax)\n",
    "    soft_targets = F.kl_div(\n",
    "        F.log_softmax(student_logits / T, dim=1),\n",
    "        F.softmax(teacher_logits / T, dim=1),\n",
    "        reduction='batchmean'\n",
    "    ) * (T * T)\n",
    "\n",
    "    # Hard label loss\n",
    "    hard_loss = F.cross_entropy(student_logits, targets)\n",
    "    return alpha * soft_targets + (1 - alpha) * hard_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtpKJq_H4jJW"
   },
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "J_snm7CV4j1x"
   },
   "outputs": [],
   "source": [
    "# Function to count trainable parameters\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Function to measure average inference latency over multiple runs\n",
    "def measure_latency(model, input_size=(1, 3, 32, 32), device='cuda', repetitions=50):\n",
    "    model.eval()\n",
    "    inputs = torch.randn(input_size).to(device)\n",
    "    with torch.no_grad():\n",
    "        # Warm-up\n",
    "        for _ in range(10):\n",
    "            _ = model(inputs)\n",
    "        # Measure\n",
    "        times = []\n",
    "        for _ in range(repetitions):\n",
    "            start = time.time()\n",
    "            _ = model(inputs)\n",
    "            end = time.time()\n",
    "            times.append(end - start)\n",
    "    return (sum(times) / repetitions) * 1000  # ms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wiv5X7eVlydg"
   },
   "source": [
    "## Train the Student via Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8GAXjD4AB49J"
   },
   "outputs": [],
   "source": [
    "def training_step(inputs, labels, teacher, student_wrapper, optimizer, device):\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        teacher_logits, teacher_feats = extract_features(teacher, inputs)\n",
    "\n",
    "    student_logits, student_feats = student_wrapper(inputs)\n",
    "    projected_feats = student_wrapper.project_features(student_feats, [t.shape for t in teacher_feats])\n",
    "    feat_loss = sum(F.mse_loss(p, t.detach()) for p, t in zip(projected_feats, teacher_feats))\n",
    "\n",
    "    loss = distillation_loss(student_logits, teacher_logits, labels) + 0.1 * feat_loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "W-DTZmLXPgK9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (student): 60.02%\n",
      "[(Training student)\tEpoch 1] Loss = 8.7847 | Val Acc = 60.02%\n",
      "✅ New best model saved.\n",
      "Accuracy (student): 65.92%\n",
      "[(Training student)\tEpoch 2] Loss = 5.9411 | Val Acc = 65.92%\n",
      "✅ New best model saved.\n",
      "Accuracy (student): 69.38%\n",
      "[(Training student)\tEpoch 3] Loss = 4.8069 | Val Acc = 69.38%\n",
      "✅ New best model saved.\n",
      "Accuracy (student): 71.84%\n",
      "[(Training student)\tEpoch 4] Loss = 4.0791 | Val Acc = 71.84%\n",
      "✅ New best model saved.\n",
      "Accuracy (student): 74.46%\n",
      "[(Training student)\tEpoch 5] Loss = 3.4702 | Val Acc = 74.46%\n",
      "✅ New best model saved.\n",
      "Accuracy (student): 71.80%\n",
      "[(Training student)\tEpoch 6] Loss = 2.9380 | Val Acc = 71.80%\n",
      "Accuracy (student): 74.02%\n",
      "[(Training student)\tEpoch 7] Loss = 2.5594 | Val Acc = 74.02%\n",
      "Accuracy (student): 74.18%\n",
      "[(Training student)\tEpoch 8] Loss = 2.2837 | Val Acc = 74.18%\n",
      "Accuracy (student): 74.72%\n",
      "[(Training student)\tEpoch 9] Loss = 1.9847 | Val Acc = 74.72%\n",
      "✅ New best model saved.\n",
      "Accuracy (student): 74.84%\n",
      "[(Training student)\tEpoch 10] Loss = 1.7854 | Val Acc = 74.84%\n",
      "✅ New best model saved.\n",
      "Accuracy (student): 75.98%\n",
      "[(Training student)\tEpoch 11] Loss = 1.6584 | Val Acc = 75.98%\n",
      "✅ New best model saved.\n",
      "Accuracy (student): 75.04%\n",
      "[(Training student)\tEpoch 12] Loss = 1.5268 | Val Acc = 75.04%\n",
      "Accuracy (student): 75.72%\n",
      "[(Training student)\tEpoch 13] Loss = 1.4189 | Val Acc = 75.72%\n",
      "Accuracy (student): 76.08%\n",
      "[(Training student)\tEpoch 14] Loss = 1.3382 | Val Acc = 76.08%\n",
      "✅ New best model saved.\n",
      "Accuracy (student): 76.50%\n",
      "[(Training student)\tEpoch 15] Loss = 1.2840 | Val Acc = 76.50%\n",
      "✅ New best model saved.\n",
      "Accuracy (student): 75.88%\n",
      "[(Training student)\tEpoch 16] Loss = 1.1921 | Val Acc = 75.88%\n",
      "Accuracy (student): 75.80%\n",
      "[(Training student)\tEpoch 17] Loss = 1.2010 | Val Acc = 75.80%\n",
      "Accuracy (student): 78.46%\n",
      "[(Training student)\tEpoch 18] Loss = 0.9510 | Val Acc = 78.46%\n",
      "✅ New best model saved.\n",
      "Accuracy (student): 78.32%\n",
      "[(Training student)\tEpoch 19] Loss = 0.7965 | Val Acc = 78.32%\n",
      "Accuracy (student): 78.30%\n",
      "[(Training student)\tEpoch 20] Loss = 0.7370 | Val Acc = 78.30%\n",
      "Accuracy (student): 79.06%\n",
      "[(Training student)\tEpoch 21] Loss = 0.7064 | Val Acc = 79.06%\n",
      "✅ New best model saved.\n",
      "Accuracy (student): 78.58%\n",
      "[(Training student)\tEpoch 22] Loss = 0.6862 | Val Acc = 78.58%\n",
      "Accuracy (student): 78.30%\n",
      "[(Training student)\tEpoch 23] Loss = 0.6790 | Val Acc = 78.30%\n",
      "Accuracy (student): 77.62%\n",
      "[(Training student)\tEpoch 24] Loss = 0.6736 | Val Acc = 77.62%\n",
      "Accuracy (student): 78.38%\n",
      "[(Training student)\tEpoch 25] Loss = 0.6742 | Val Acc = 78.38%\n",
      "Accuracy (student): 78.34%\n",
      "[(Training student)\tEpoch 26] Loss = 0.6626 | Val Acc = 78.34%\n",
      "Accuracy (student): 78.36%\n",
      "[(Training student)\tEpoch 27] Loss = 0.6593 | Val Acc = 78.36%\n",
      "Accuracy (student): 78.24%\n",
      "[(Training student)\tEpoch 28] Loss = 0.6728 | Val Acc = 78.24%\n",
      "Accuracy (student): 78.34%\n",
      "[(Training student)\tEpoch 29] Loss = 0.6521 | Val Acc = 78.34%\n",
      "Accuracy (student): 78.16%\n",
      "[(Training student)\tEpoch 30] Loss = 0.6344 | Val Acc = 78.16%\n",
      "Accuracy (student): 78.42%\n",
      "[(Training student)\tEpoch 31] Loss = 0.6126 | Val Acc = 78.42%\n",
      "Accuracy (student): 79.14%\n",
      "[(Training student)\tEpoch 32] Loss = 0.5486 | Val Acc = 79.14%\n",
      "✅ New best model saved.\n",
      "Accuracy (student): 78.80%\n",
      "[(Training student)\tEpoch 33] Loss = 0.5059 | Val Acc = 78.80%\n",
      "Accuracy (student): 78.78%\n",
      "[(Training student)\tEpoch 34] Loss = 0.4874 | Val Acc = 78.78%\n",
      "Accuracy (student): 79.18%\n",
      "[(Training student)\tEpoch 35] Loss = 0.4807 | Val Acc = 79.18%\n",
      "✅ New best model saved.\n",
      "Accuracy (student): 78.94%\n",
      "[(Training student)\tEpoch 36] Loss = 0.4735 | Val Acc = 78.94%\n",
      "Accuracy (student): 78.64%\n",
      "[(Training student)\tEpoch 37] Loss = 0.4723 | Val Acc = 78.64%\n",
      "Accuracy (student): 78.98%\n",
      "[(Training student)\tEpoch 38] Loss = 0.4516 | Val Acc = 78.98%\n",
      "Accuracy (student): 78.74%\n",
      "[(Training student)\tEpoch 39] Loss = 0.4360 | Val Acc = 78.74%\n",
      "Accuracy (student): 78.82%\n",
      "[(Training student)\tEpoch 40] Loss = 0.4319 | Val Acc = 78.82%\n",
      "Accuracy (student): 78.46%\n",
      "[(Training student)\tEpoch 41] Loss = 0.4267 | Val Acc = 78.46%\n",
      "Accuracy (student): 78.86%\n",
      "[(Training student)\tEpoch 42] Loss = 0.4241 | Val Acc = 78.86%\n",
      "Accuracy (student): 78.64%\n",
      "[(Training student)\tEpoch 43] Loss = 0.4151 | Val Acc = 78.64%\n",
      "Accuracy (student): 78.64%\n",
      "[(Training student)\tEpoch 44] Loss = 0.4075 | Val Acc = 78.64%\n",
      "Accuracy (student): 78.96%\n",
      "[(Training student)\tEpoch 45] Loss = 0.4064 | Val Acc = 78.96%\n",
      "Accuracy (student): 78.80%\n",
      "[(Training student)\tEpoch 46] Loss = 0.4035 | Val Acc = 78.80%\n",
      "Accuracy (student): 78.78%\n",
      "[(Training student)\tEpoch 47] Loss = 0.4018 | Val Acc = 78.78%\n",
      "Accuracy (student): 78.58%\n",
      "[(Training student)\tEpoch 48] Loss = 0.3996 | Val Acc = 78.58%\n",
      "Accuracy (student): 78.66%\n",
      "[(Training student)\tEpoch 49] Loss = 0.3960 | Val Acc = 78.66%\n",
      "Accuracy (student): 78.62%\n",
      "[(Training student)\tEpoch 50] Loss = 0.3931 | Val Acc = 78.62%\n"
     ]
    }
   ],
   "source": [
    "# Train the student using the teacher's output as soft targets\n",
    "teacher.eval()  # Teacher in eval mode\n",
    "\n",
    "if debug:\n",
    "  num_epochs = 1\n",
    "else:\n",
    "  num_epochs = 25\n",
    "\n",
    "best_val_acc = 0.0\n",
    "save_path = \"student_distilled.pth\"\n",
    "\n",
    "# Reduce LR if validation loss doesn't improve for 3 epochs\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    student_wrapper.train()\n",
    "    running_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        loss = training_step(inputs, labels, teacher, student_wrapper, optimizer, device)\n",
    "        running_loss += loss\n",
    "\n",
    "    val_acc = evaluate_accuracy(student_wrapper.model, val_loader, \"student\")\n",
    "    print(f\"[(Training student)\\tEpoch {epoch+1}] Loss = {running_loss/len(train_loader):.4f} | Val Acc = {val_acc*100:.2f}%\")\n",
    "    scheduler.step(loss)\n",
    "\n",
    "    # Save best\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(student_wrapper.state_dict(), save_path)\n",
    "        print(\"✅ New best model saved.\")\n",
    "\n",
    "# load best checkpoint\n",
    "student_wrapper.load_state_dict(torch.load(save_path))\n",
    "student = student_wrapper.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_J2dwzIl1ss"
   },
   "source": [
    "## Model Comparison Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ldidj0f_l4P0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (teacher on testset): 85.10%\n",
      "Accuracy (student on testset): 79.24%\n",
      "Teacher Params: 23.53M\n",
      "Student Params: 11.18M\n",
      "Teacher Latency: 3.82 ms\n",
      "Student Latency: 1.54 ms\n",
      "Teacher Test Accuracy: 85.10%\n",
      "Student Test Accuracy: 79.24%\n"
     ]
    }
   ],
   "source": [
    "# Compare size, latency, and accuracy\n",
    "teacher_params = count_params(teacher)\n",
    "student_params = count_params(student)\n",
    "\n",
    "teacher_latency = measure_latency(teacher, device=device)\n",
    "student_latency = measure_latency(student, device=device)\n",
    "\n",
    "teacher_acc = evaluate_accuracy(teacher, test_loader, \"teacher on testset\")\n",
    "student_acc = evaluate_accuracy(student, test_loader, \"student on testset\")\n",
    "\n",
    "print(f\"Teacher Params: {teacher_params / 1e6:.2f}M\")\n",
    "print(f\"Student Params: {student_params / 1e6:.2f}M\")\n",
    "print(f\"Teacher Latency: {teacher_latency:.2f} ms\")\n",
    "print(f\"Student Latency: {student_latency:.2f} ms\")\n",
    "print(f\"Teacher Test Accuracy: {teacher_acc * 100:.2f}%\")\n",
    "print(f\"Student Test Accuracy: {student_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HYg6HZDivYK1"
   },
   "outputs": [],
   "source": [
    "# Bad-Student: ResNet18 training from scratch on its own, re-headed for CIFAR-10\n",
    "bad_student = models.resnet18(weights=None)\n",
    "bad_student.fc = nn.Linear(512, 10).to(device)\n",
    "bad_student = bad_student.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Bad-student)\tEpoch 1: loss=1.0124\n",
      "Accuracy (Epoch 1): 58.06%\n",
      "(Bad-student)\tEpoch 2: loss=0.8091\n",
      "Accuracy (Epoch 2): 63.10%\n",
      "(Bad-student)\tEpoch 3: loss=0.8210\n",
      "Accuracy (Epoch 3): 68.06%\n",
      "(Bad-student)\tEpoch 4: loss=0.7438\n",
      "Accuracy (Epoch 4): 68.74%\n",
      "(Bad-student)\tEpoch 5: loss=0.8731\n",
      "Accuracy (Epoch 5): 71.18%\n",
      "(Bad-student)\tEpoch 6: loss=0.7033\n",
      "Accuracy (Epoch 6): 72.70%\n",
      "(Bad-student)\tEpoch 7: loss=0.3875\n",
      "Accuracy (Epoch 7): 71.32%\n",
      "(Bad-student)\tEpoch 8: loss=0.3319\n",
      "Accuracy (Epoch 8): 72.06%\n",
      "(Bad-student)\tEpoch 9: loss=0.3094\n",
      "Accuracy (Epoch 9): 73.80%\n",
      "(Bad-student)\tEpoch 10: loss=0.0464\n",
      "Accuracy (Epoch 10): 73.28%\n",
      "(Bad-student)\tEpoch 11: loss=0.3143\n",
      "Accuracy (Epoch 11): 72.76%\n",
      "(Bad-student)\tEpoch 12: loss=0.1305\n",
      "Accuracy (Epoch 12): 72.06%\n",
      "(Bad-student)\tEpoch 13: loss=0.2509\n",
      "Accuracy (Epoch 13): 73.52%\n",
      "(Bad-student)\tEpoch 14: loss=0.1958\n",
      "Accuracy (Epoch 14): 73.06%\n",
      "(Bad-student)\tEpoch 15: loss=0.0951\n",
      "Accuracy (Epoch 15): 72.90%\n",
      "(Bad-student)\tEpoch 16: loss=0.0546\n",
      "Accuracy (Epoch 16): 73.58%\n",
      "(Bad-student)\tEpoch 17: loss=0.0727\n",
      "Accuracy (Epoch 17): 74.32%\n",
      "(Bad-student)\tEpoch 18: loss=0.0300\n",
      "Accuracy (Epoch 18): 74.38%\n",
      "(Bad-student)\tEpoch 19: loss=0.0327\n",
      "Accuracy (Epoch 19): 73.60%\n",
      "(Bad-student)\tEpoch 20: loss=0.1267\n",
      "Accuracy (Epoch 20): 73.14%\n",
      "Training complete. Model saved to bad_student.pth\n",
      "Saved fine-tuned teacher.\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "bad_student = train_teacher(bad_student, train_loader, epochs=epochs, tag=\"Bad-student\", save_path=\"bad_student.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (bad_student on testset): 73.79%\n"
     ]
    }
   ],
   "source": [
    "bad_student_acc = evaluate_accuracy(bad_student, test_loader, \"bad_student on testset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Bad-student)\tEpoch 1: loss=0.0844\n",
      "Accuracy (Epoch 1): 73.80%\n",
      "(Bad-student)\tEpoch 2: loss=0.0631\n",
      "Accuracy (Epoch 2): 72.42%\n",
      "(Bad-student)\tEpoch 3: loss=0.0556\n",
      "Accuracy (Epoch 3): 73.86%\n",
      "(Bad-student)\tEpoch 4: loss=0.1145\n",
      "Accuracy (Epoch 4): 73.42%\n",
      "(Bad-student)\tEpoch 5: loss=0.0202\n",
      "Accuracy (Epoch 5): 74.10%\n",
      "Training complete. Model saved to bad_student.pth\n",
      "Saved fine-tuned teacher.\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "bad_student = train_teacher(bad_student, train_loader, epochs=epochs, tag=\"Bad-student\", save_path=\"bad_student.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
