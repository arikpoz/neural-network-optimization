{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_long_operations = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Quantization - Quantization Aware Training (QAT) using PyTorch 2 Export Quantization and X86 Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** The following quantization code is not for the faint of heart. Since this is PyTorch’s third iteration of a quantization API, the only thing we can say with confidence is that it will change again. That said, it’s been tested and confirmed to work on the following PyTorch versions:\n",
    "- 2.8.0.dev20250319+cu128\n",
    "- 2.6.0+cu124\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mC5uxAf3rqhI"
   },
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 183,
     "status": "error",
     "timestamp": 1744640020045,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "kVSqC5hmrOyu",
    "outputId": "d0206420-555b-4e57-9731-721e98bf4672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.quantization import quantize_dynamic\n",
    "from torch.ao.quantization import get_default_qconfig, QConfigMapping\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# ignores irrelevant warning, see: https://github.com/pytorch/pytorch/issues/149829\n",
    "warnings.filterwarnings(\"ignore\", message=\".*TF32 acceleration on top of oneDNN is available for Intel GPUs. The current Torch version does not have Intel GPU Support.*\")\n",
    "\n",
    "# ignores irrelevant warning, see: https://github.com/tensorflow/tensorflow/issues/77293\n",
    "warnings.filterwarnings(\"ignore\", message=\".*erase_node(.*) on an already erased node.*\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"{device=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7b_v0s1rj9k"
   },
   "source": [
    "## Get CIFAR-10 train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4716,
     "status": "ok",
     "timestamp": 1744637133096,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "DMTbF6hHr8TP"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(\n",
    "    datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform),\n",
    "    batch_size=128, shuffle=True\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(\n",
    "    datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform),\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "calibration_dataset = Subset(train_dataset, range(256))\n",
    "calibration_loader = DataLoader(calibration_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHLEP6wFsHWs"
   },
   "source": [
    "## Adjust ResNet18 network for CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1744637136962,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "LUYEnQOysJrg"
   },
   "outputs": [],
   "source": [
    "def get_resnet18_for_cifar10():\n",
    "    model = models.resnet18(weights=None, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model.to(device)\n",
    "\n",
    "model_to_quantize = get_resnet18_for_cifar10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-556Tm4EsRlF"
   },
   "source": [
    "## Define Train and Evaluate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1744637138735,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "w_Uo6-mFsShj"
   },
   "outputs": [],
   "source": [
    "def train(model, loader, epochs, lr=0.01, save_path=\"model.pth\", silent=False):\n",
    "    if os.path.exists(save_path):\n",
    "        if not silent:\n",
    "            print(f\"Model already trained. Loading from {save_path}\")\n",
    "        model.load_state_dict(torch.load(save_path))\n",
    "        return\n",
    "\n",
    "    # no saved model found. training from given model state\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(x), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if not silent:\n",
    "            print(f\"Epoch {epoch+1}: loss={loss.item():.4f}\")\n",
    "\n",
    "    if save_path:\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        if not silent:\n",
    "            print(f\"Training complete. Model saved to {save_path}\")\n",
    "\n",
    "def evaluate(model, device_str):\n",
    "    if device_str:\n",
    "        device = torch.device(device_str)\n",
    "        model.to(device)\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            if device_str:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "            preds = model(x).argmax(1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0CGz0lVshYc"
   },
   "source": [
    "## Define helper functions to measure latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1744637141931,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "4i7uFUWHsbNL"
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"\n",
    "    A simple timer utility for measuring elapsed time in milliseconds.\n",
    "\n",
    "    Supports both GPU and CPU timing:\n",
    "    - If CUDA is available, uses torch.cuda.Event for accurate GPU timing.\n",
    "    - Otherwise, falls back to wall-clock CPU timing via time.time().\n",
    "\n",
    "    Methods:\n",
    "        start(): Start the timer.\n",
    "        stop(): Stop the timer and return the elapsed time in milliseconds.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.starter = torch.cuda.Event(enable_timing=True)\n",
    "            self.ender = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    def start(self):\n",
    "        if self.use_cuda:\n",
    "            self.starter.record()\n",
    "        else:\n",
    "            self.start_time = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        if self.use_cuda:\n",
    "            self.ender.record()\n",
    "            torch.cuda.synchronize()\n",
    "            return self.starter.elapsed_time(self.ender)  # ms\n",
    "        else:\n",
    "            return (time.time() - self.start_time) * 1000  # ms\n",
    "\n",
    "def estimate_latency(model, example_inputs, repetitions=50):\n",
    "    timer = Timer()\n",
    "    timings = np.zeros((repetitions, 1))\n",
    "\n",
    "    # warm-up\n",
    "    for _ in range(5):\n",
    "        _ = model(example_inputs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            timer.start()\n",
    "            _ = model(example_inputs)\n",
    "            elapsed = timer.stop()\n",
    "            timings[rep] = elapsed\n",
    "\n",
    "    return np.mean(timings), np.std(timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print(\"Size (MB):\", os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove(\"temp.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1JV1s-ssscv"
   },
   "source": [
    "## Train full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1744637145430,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "FNO_oyLGsmT0",
    "outputId": "e3a9b9cb-b550-4581-ef4f-8a959f8fd3dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already trained. Loading from full_model.pth\n"
     ]
    }
   ],
   "source": [
    "model_to_quantize.train()\n",
    "train(model_to_quantize, train_loader, epochs=10, save_path=\"full_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFS44nQpwg9Z"
   },
   "source": [
    "## Evaluate full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 100206,
     "status": "ok",
     "timestamp": 1744636572612,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "j5fVkI49wgPG",
    "outputId": "d73ca6c2-6840-456f-b7df-6d20dea66b07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (full): 79.31%\n",
      "Size (full): 44.78 MB\n",
      "Latency (full, on gpu): 16.82 ± 0.03 ms\n",
      "Skipped CPU evaluation\n"
     ]
    }
   ],
   "source": [
    "# evaluate accuracy\n",
    "model_to_quantize.eval()\n",
    "accuracy_full = evaluate(model_to_quantize, 'cuda')\n",
    "print(f\"Accuracy (full): {accuracy_full*100:.2f}%\")\n",
    "\n",
    "# get model size\n",
    "size_mb_full = os.path.getsize(\"full_model.pth\") / 1e6\n",
    "print(f\"Size (full): {size_mb_full:.2f} MB\")\n",
    "\n",
    "# estimate latency on GPU\n",
    "example_input = torch.rand(128, 3, 32, 32).cuda()\n",
    "model_to_quantize.cuda()\n",
    "latency_mu_full_gpu, latency_std_full_gpu = estimate_latency(model_to_quantize, example_input)\n",
    "print(f\"Latency (full, on gpu): {latency_mu_full_gpu:.2f} ± {latency_std_full_gpu:.2f} ms\")\n",
    "\n",
    "# estimate latency on CPU\n",
    "if not skip_long_operations:\n",
    "    example_input = torch.rand(128, 3, 32, 32).cpu()\n",
    "    model_to_quantize.cpu()\n",
    "    latency_mu_full_cpu, latency_std_full_cpu = estimate_latency(model_to_quantize, example_input)\n",
    "    print(f\"Latency (full, on cpu): {latency_mu_full_cpu:.2f} ± {latency_std_full_cpu:.2f} ms\")\n",
    "else:\n",
    "    print(\"Skipped CPU evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization Aware Training (QAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_quantize = model_to_quantize.eval()\n",
    "model_to_quantize = model_to_quantize.to(device)\n",
    "\n",
    "# program capture\n",
    "example_inputs = (torch.rand(128, 3, 32, 32).to(device),)\n",
    "exported_model  = torch.export.export_for_training(model_to_quantize, example_inputs).module()\n",
    "\n",
    "# quantization\n",
    "from torch.ao.quantization.quantize_pt2e import (\n",
    "  prepare_qat_pt2e,\n",
    "  convert_pt2e,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization.quantizer.xnnpack_quantizer import (\n",
    "  XNNPACKQuantizer,\n",
    "  get_symmetric_quantization_config,\n",
    ")\n",
    "\n",
    "quantizer = XNNPACKQuantizer()\n",
    "quantizer.set_global(get_symmetric_quantization_config(is_qat=True))\n",
    "\n",
    "prepared_model = prepare_qat_pt2e(exported_model, quantizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already trained. Loading from qat_model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.ao.quantization.move_exported_model_to_train(prepared_model)\n",
    "train(prepared_model, train_loader, epochs=3, save_path=\"qat_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "quantized_model = convert_pt2e(prepared_model)\n",
    "\n",
    "# we have a model with aten ops doing integer computations when possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of baseline model\n",
      "Size (MB): 44.767812\n",
      "Baseline Float Model Evaluation accuracy: 80.84\n",
      "Size of model after quantization\n",
      "Size (MB): 11.195534\n",
      "[before serilaization] Evaluation accuracy on test dataset: 80.54\n"
     ]
    }
   ],
   "source": [
    "# Baseline model size and accuracy\n",
    "print(\"Size of baseline model\")\n",
    "print_size_of_model(model_to_quantize)\n",
    "\n",
    "model_to_quantize.eval()\n",
    "accuracy_baseline = evaluate(model_to_quantize, device_str = 'cuda')\n",
    "print(\"Baseline Float Model Evaluation accuracy: %2.2f\"%(100*accuracy_baseline))\n",
    "\n",
    "# Quantized model size and accuracy\n",
    "print(\"Size of model after quantization\")\n",
    "# export again to remove unused weights\n",
    "quantized_model = torch.export.export_for_training(quantized_model, example_inputs).module()\n",
    "print_size_of_model(quantized_model)\n",
    "\n",
    "torch.ao.quantization.move_exported_model_to_eval(quantized_model)\n",
    "accuracy_quantized = evaluate(quantized_model, device_str = 'cuda')\n",
    "print(\"[before serilaization] Evaluation accuracy on test dataset: %2.2f\"%(100*accuracy_quantized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. PyTorch Documentation: [Quantization](https://pytorch.org/docs/stable/quantization.html)\n",
    "2. PyTorch Documentation: [PyTorch 2 Export Post Training Quantization](https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html)\n",
    "3. PyTorch Documentation: [PyTorch 2 Export Quantization-Aware Training (QAT)](https://pytorch.org/tutorials/prototype/pt2e_quant_qat.html)\n",
    "4. PyTorch Documentation: [PyTorch 2 Export Quantization with X86 Backend through Inductor](https://pytorch.org/tutorials/prototype/pt2e_quant_x86_inductor.html)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOJ7eNLZPT5dGBuCMUjogIL",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
