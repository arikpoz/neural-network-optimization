{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Quantization - Post Training Quantization (PTQ) using PyTorch 2 Export Quantization and XNNPACK Quantizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mC5uxAf3rqhI"
   },
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 183,
     "status": "error",
     "timestamp": 1744640020045,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "kVSqC5hmrOyu",
    "outputId": "d0206420-555b-4e57-9731-721e98bf4672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.8.0.dev20250319+cu128\n",
      "Device used: cuda\n",
      "Should skip CPU evaluations: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from packaging import version\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.quantization import quantize_dynamic\n",
    "from torch.ao.quantization import get_default_qconfig, QConfigMapping\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# ignores irrelevant warning, see: https://github.com/pytorch/pytorch/issues/149829\n",
    "warnings.filterwarnings(\"ignore\", message=\".*TF32 acceleration on top of oneDNN is available for Intel GPUs. The current Torch version does not have Intel GPU Support.*\")\n",
    "\n",
    "# ignores irrelevant warning, see: https://github.com/tensorflow/tensorflow/issues/77293\n",
    "warnings.filterwarnings(\"ignore\", message=\".*erase_node(.*) on an already erased node.*\")\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device used: {device.type}\")\n",
    "\n",
    "skip_cpu = False # change to True to skip the slow checks on CPU\n",
    "print(f\"Should skip CPU evaluations: {skip_cpu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7b_v0s1rj9k"
   },
   "source": [
    "## Get CIFAR-10 train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4716,
     "status": "ok",
     "timestamp": 1744637133096,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "DMTbF6hHr8TP"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(\n",
    "    datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform),\n",
    "    batch_size=128, shuffle=True\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(\n",
    "    datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform),\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "calibration_dataset = Subset(train_dataset, range(256))\n",
    "calibration_loader = DataLoader(calibration_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHLEP6wFsHWs"
   },
   "source": [
    "## Adjust ResNet18 network for CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1744637136962,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "LUYEnQOysJrg"
   },
   "outputs": [],
   "source": [
    "def get_resnet18_for_cifar10():\n",
    "    \"\"\"\n",
    "    Returns a ResNet-18 model adjusted for CIFAR-10:\n",
    "    - 3x3 conv with stride 1\n",
    "    - No max pooling\n",
    "    - 10 output classes\n",
    "    \"\"\"\n",
    "    model = models.resnet18(weights=None, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model.to(device)\n",
    "\n",
    "model_to_quantize = get_resnet18_for_cifar10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-556Tm4EsRlF"
   },
   "source": [
    "## Define Train and Evaluate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1744637138735,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "w_Uo6-mFsShj"
   },
   "outputs": [],
   "source": [
    "def train(model, loader, epochs, lr=0.01, save_path=\"model.pth\", silent=False):\n",
    "    \"\"\"\n",
    "    Trains a model with SGD and cross-entropy loss.\n",
    "    Loads from save_path if it exists.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        model.train()\n",
    "    except NotImplementedError:\n",
    "        torch.ao.quantization.move_exported_model_to_train(model)\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        if not silent:\n",
    "            print(f\"Model already trained. Loading from {save_path}\")\n",
    "        model.load_state_dict(torch.load(save_path))\n",
    "        return\n",
    "\n",
    "    # no saved model found. training from given model state\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(x), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if not silent:\n",
    "            print(f\"Epoch {epoch+1}: loss={loss.item():.4f}\")\n",
    "            evaluate(model, f\"Epoch {epoch+1}\")\n",
    "            try:\n",
    "                model.train()\n",
    "            except NotImplementedError:\n",
    "                torch.ao.quantization.move_exported_model_to_train(model)\n",
    "\n",
    "    if save_path:\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        if not silent:\n",
    "            print(f\"Training complete. Model saved to {save_path}\")\n",
    "\n",
    "def evaluate(model, tag):\n",
    "    \"\"\"\n",
    "    Evaluates the model on test_loader and prints accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        model.eval()\n",
    "    except NotImplementedError:\n",
    "        model = torch.ao.quantization.move_exported_model_to_eval(model)\n",
    "\n",
    "    model.to(device)\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            preds = model(x).argmax(1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy ({tag}): {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0CGz0lVshYc"
   },
   "source": [
    "## Define helper functions to measure latency and model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1744637141931,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "4i7uFUWHsbNL"
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"\n",
    "    A simple timer utility for measuring elapsed time in milliseconds.\n",
    "\n",
    "    Supports both GPU and CPU timing:\n",
    "    - If CUDA is available, uses torch.cuda.Event for accurate GPU timing.\n",
    "    - Otherwise, falls back to wall-clock CPU timing via time.time().\n",
    "\n",
    "    Methods:\n",
    "        start(): Start the timer.\n",
    "        stop(): Stop the timer and return the elapsed time in milliseconds.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.starter = torch.cuda.Event(enable_timing=True)\n",
    "            self.ender = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    def start(self):\n",
    "        if self.use_cuda:\n",
    "            self.starter.record()\n",
    "        else:\n",
    "            self.start_time = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        if self.use_cuda:\n",
    "            self.ender.record()\n",
    "            torch.cuda.synchronize()\n",
    "            return self.starter.elapsed_time(self.ender)  # ms\n",
    "        else:\n",
    "            return (time.time() - self.start_time) * 1000  # ms\n",
    "\n",
    "def estimate_latency(model, example_inputs, repetitions=50):\n",
    "    \"\"\"\n",
    "    Returns avg and std inference latency (ms) over given runs.\n",
    "    \"\"\"\n",
    "    \n",
    "    timer = Timer()\n",
    "    timings = np.zeros((repetitions, 1))\n",
    "\n",
    "    # warm-up\n",
    "    for _ in range(5):\n",
    "        _ = model(example_inputs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            timer.start()\n",
    "            _ = model(example_inputs)\n",
    "            elapsed = timer.stop()\n",
    "            timings[rep] = elapsed\n",
    "\n",
    "    return np.mean(timings), np.std(timings)\n",
    "\n",
    "def estimate_latency_full(model, tag, skip_cpu):\n",
    "    \"\"\"\n",
    "    Prints model latency on GPU and (optionally) CPU.\n",
    "    \"\"\"\n",
    "\n",
    "    # estimate latency on CPU\n",
    "    if not skip_cpu:\n",
    "        example_input = torch.rand(128, 3, 32, 32).cpu()\n",
    "        model.cpu()\n",
    "        latency_mu, latency_std = estimate_latency(model, example_input)\n",
    "        print(f\"Latency ({tag}, on CPU): {latency_mu:.2f} ± {latency_std:.2f} ms\")\n",
    "\n",
    "    # estimate latency on GPU\n",
    "    example_input = torch.rand(128, 3, 32, 32).cuda()\n",
    "    model.cuda()\n",
    "    latency_mu, latency_std = estimate_latency(model, example_input)\n",
    "    print(f\"Latency ({tag}, on GPU): {latency_mu:.2f} ± {latency_std:.2f} ms\")\n",
    "\n",
    "def print_size_of_model(model, tag=\"\"):\n",
    "    \"\"\"\n",
    "    Prints model size (MB).\n",
    "    \"\"\"\n",
    "    \n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size_mb_full = os.path.getsize(\"temp.p\") / 1e6\n",
    "    print(f\"Size ({tag}): {size_mb_full:.2f} MB\")\n",
    "    os.remove(\"temp.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1JV1s-ssscv"
   },
   "source": [
    "## Train full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1744637145430,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "FNO_oyLGsmT0",
    "outputId": "e3a9b9cb-b550-4581-ef4f-8a959f8fd3dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.8905\n",
      "Accuracy (Epoch 1): 58.78%\n",
      "Epoch 2: loss=0.7371\n",
      "Accuracy (Epoch 2): 67.51%\n",
      "Epoch 3: loss=0.7273\n",
      "Accuracy (Epoch 3): 74.37%\n",
      "Epoch 4: loss=0.5133\n",
      "Accuracy (Epoch 4): 74.88%\n",
      "Epoch 5: loss=0.2877\n",
      "Accuracy (Epoch 5): 74.88%\n",
      "Epoch 6: loss=0.3319\n",
      "Accuracy (Epoch 6): 67.36%\n",
      "Epoch 7: loss=0.2166\n",
      "Accuracy (Epoch 7): 73.47%\n",
      "Epoch 8: loss=0.1160\n",
      "Accuracy (Epoch 8): 76.27%\n",
      "Epoch 9: loss=0.1579\n",
      "Accuracy (Epoch 9): 74.73%\n",
      "Epoch 10: loss=0.0222\n",
      "Accuracy (Epoch 10): 77.65%\n",
      "Epoch 11: loss=0.0090\n",
      "Accuracy (Epoch 11): 78.60%\n",
      "Epoch 12: loss=0.0024\n",
      "Accuracy (Epoch 12): 79.19%\n",
      "Epoch 13: loss=0.0005\n",
      "Accuracy (Epoch 13): 80.18%\n",
      "Epoch 14: loss=0.0002\n",
      "Accuracy (Epoch 14): 80.48%\n",
      "Epoch 15: loss=0.0005\n",
      "Accuracy (Epoch 15): 80.53%\n",
      "Training complete. Model saved to full_model.pth\n"
     ]
    }
   ],
   "source": [
    "train(model_to_quantize, train_loader, epochs=15, save_path=\"full_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFS44nQpwg9Z"
   },
   "source": [
    "## Evaluate full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 100206,
     "status": "ok",
     "timestamp": 1744636572612,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "j5fVkI49wgPG",
    "outputId": "d73ca6c2-6840-456f-b7df-6d20dea66b07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (full): 44.77 MB\n",
      "Accuracy (full): 80.53%\n",
      "Latency (full, on CPU): 750.06 ± 76.23 ms\n",
      "Latency (full, on GPU): 16.76 ± 0.32 ms\n"
     ]
    }
   ],
   "source": [
    "# get full model size\n",
    "print_size_of_model(model_to_quantize, \"full\")\n",
    "\n",
    "# evaluate full accuracy\n",
    "accuracy_full = evaluate(model_to_quantize, 'full')\n",
    "\n",
    "# estimate full model latency\n",
    "estimate_latency_full(model_to_quantize, 'full', skip_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Training Quantization (PTQ) with XNNPACK Quantizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** The following quantization code is not for the faint of heart. Since this is PyTorch’s third iteration of a quantization API, the only thing we can say with confidence is that it will change again. That said, it’s been tested and confirmed to work on the following PyTorch versions:\n",
    "- 2.8.0.dev20250319+cu128\n",
    "- 2.6.0+cu124\n",
    "- 2.4.1+cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/quantizer/xnnpack_quantizer.py:281: UserWarning: XNNPACKQuantizer is deprecated!\n",
      "  warnings.warn(f\"{self.__class__.__name__} is deprecated!\")\n"
     ]
    }
   ],
   "source": [
    "from torch.ao.quantization.quantize_pt2e import (\n",
    "  prepare_pt2e,\n",
    "  convert_pt2e,\n",
    ")\n",
    "\n",
    "from torch.ao.quantization.quantizer.xnnpack_quantizer import (\n",
    "  XNNPACKQuantizer,\n",
    "  get_symmetric_quantization_config,\n",
    ")\n",
    "\n",
    "# batch of 128 images, each with 3 color channels and 32x32 resolution (CIFAR-10)\n",
    "example_inputs = (torch.rand(128, 3, 32, 32).to(device),)\n",
    "\n",
    "# export the model to a standardized format before quantization\n",
    "if version.parse(torch.__version__) >= version.parse(\"2.5\"): # for pytorch 2.5+\n",
    "    exported_model  = torch.export.export_for_training(model_to_quantize, example_inputs).module()\n",
    "else: # for pytorch 2.4\n",
    "    from torch._export import capture_pre_autograd_graph\n",
    "    exported_model = capture_pre_autograd_graph(model_to_quantize, example_inputs) \n",
    "\n",
    "# quantization setup for XNNPACK quantizer\n",
    "quantizer = XNNPACKQuantizer()\n",
    "quantizer.set_global(get_symmetric_quantization_config())\n",
    "\n",
    "# preparing for PTQ by folding batch-norm into preceding conv2d operators, and inserting observers in appropriate places\n",
    "prepared_model = prepare_pt2e(exported_model, quantizer)\n",
    "\n",
    "# run inference on calibration data to collect activation stats needed for activation quantization\n",
    "def calibrate(model, data_loader):\n",
    "    torch.ao.quantization.move_exported_model_to_eval(model)\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            model(image.to(device))\n",
    "calibrate(prepared_model, calibration_loader)\n",
    "\n",
    "# converts calibrated model to a quantized model\n",
    "quantized_model = convert_pt2e(prepared_model)\n",
    "\n",
    "# export again to remove unused weights after quantization\n",
    "if version.parse(torch.__version__) >= version.parse(\"2.5\"): # for pytorch 2.5+\n",
    "    quantized_model = torch.export.export_for_training(quantized_model, example_inputs).module()\n",
    "else: # for pytorch 2.4\n",
    "    quantized_model = capture_pre_autograd_graph(quantized_model, example_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (quantized): 11.20 MB\n",
      "Accuracy (quantized): 80.41%\n",
      "Latency (quantized, on CPU): 2310.11 ± 116.63 ms\n",
      "Latency (quantized, on GPU): 43.51 ± 0.23 ms\n"
     ]
    }
   ],
   "source": [
    "# get quantized model size\n",
    "print_size_of_model(quantized_model, \"quantized\")\n",
    "\n",
    "# evaluate quantized accuracy\n",
    "accuracy_full = evaluate(quantized_model, 'quantized')\n",
    "\n",
    "# estimate quantized model latency\n",
    "estimate_latency_full(quantized_model, 'quantized', skip_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. PyTorch Documentation: [Quantization](https://pytorch.org/docs/stable/quantization.html)\n",
    "2. PyTorch Documentation: [PyTorch 2 Export Post Training Quantization](https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html)\n",
    "3. PyTorch Documentation: [PyTorch 2 Export Quantization-Aware Training (QAT)](https://pytorch.org/tutorials/prototype/pt2e_quant_qat.html)\n",
    "4. PyTorch Documentation: [PyTorch 2 Export Quantization with X86 Backend through Inductor](https://pytorch.org/tutorials/prototype/pt2e_quant_x86_inductor.html)\n",
    "5. PyTorch Dev Discussions: [TorchInductor Update 6: CPU backend performance update and new features in PyTorch 2.1](https://dev-discuss.pytorch.org/t/torchinductor-update-6-cpu-backend-performance-update-and-new-features-in-pytorch-2-1/1514)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOJ7eNLZPT5dGBuCMUjogIL",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
