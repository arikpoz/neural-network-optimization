{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mC5uxAf3rqhI"
   },
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 183,
     "status": "error",
     "timestamp": 1744640020045,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "kVSqC5hmrOyu",
    "outputId": "d0206420-555b-4e57-9731-721e98bf4672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.quantization import quantize_dynamic\n",
    "from torch.ao.quantization import get_default_qconfig, QConfigMapping\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"{device=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7b_v0s1rj9k"
   },
   "source": [
    "## Get CIFAR-10 train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4716,
     "status": "ok",
     "timestamp": 1744637133096,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "DMTbF6hHr8TP"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(\n",
    "    datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform),\n",
    "    batch_size=128, shuffle=True\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(\n",
    "    datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform),\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "calibration_dataset = Subset(train_dataset, range(256))\n",
    "calibration_loader = DataLoader(calibration_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHLEP6wFsHWs"
   },
   "source": [
    "## Adjust ResNet18 network for CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1744637136962,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "LUYEnQOysJrg"
   },
   "outputs": [],
   "source": [
    "def get_resnet18_for_cifar10():\n",
    "    model = models.resnet18(weights=None, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model.to(device)\n",
    "\n",
    "full_model = get_resnet18_for_cifar10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-556Tm4EsRlF"
   },
   "source": [
    "## Define Train and Evaluate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1744637138735,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "w_Uo6-mFsShj"
   },
   "outputs": [],
   "source": [
    "def train(model, loader, epochs, lr=0.01, save_path=\"model.pth\", silent=False):\n",
    "    if os.path.exists(save_path):\n",
    "        if not silent:\n",
    "            print(f\"Model already trained. Loading from {save_path}\")\n",
    "        model.load_state_dict(torch.load(save_path))\n",
    "        return\n",
    "\n",
    "    # no saved model found. training from given model state\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(x), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if not silent:\n",
    "            print(f\"Epoch {epoch+1}: loss={loss.item():.4f}\")\n",
    "\n",
    "    if save_path:\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        if not silent:\n",
    "            print(f\"Training complete. Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1744637140599,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "d1J1CwYNsJhc"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, device_str):\n",
    "    model.eval()\n",
    "    if device_str:\n",
    "        device = torch.device(device_str)\n",
    "        model.to(device)\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            if device_str:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "            preds = model(x).argmax(1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0CGz0lVshYc"
   },
   "source": [
    "## Define helper functions to measure latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1744637141931,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "4i7uFUWHsbNL"
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.starter = torch.cuda.Event(enable_timing=True)\n",
    "            self.ender = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    def start(self):\n",
    "        if self.use_cuda:\n",
    "            self.starter.record()\n",
    "        else:\n",
    "            self.start_time = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        if self.use_cuda:\n",
    "            self.ender.record()\n",
    "            torch.cuda.synchronize()\n",
    "            return self.starter.elapsed_time(self.ender)  # ms\n",
    "        else:\n",
    "            return (time.time() - self.start_time) * 1000  # ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1744637142613,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "DXg0Jc2msbFz"
   },
   "outputs": [],
   "source": [
    "def estimate_latency(model, example_inputs, repetitions=50):\n",
    "    timer = Timer()\n",
    "    timings = np.zeros((repetitions, 1))\n",
    "\n",
    "    # warm-up\n",
    "    for _ in range(5):\n",
    "        _ = model(example_inputs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            timer.start()\n",
    "            _ = model(example_inputs)\n",
    "            elapsed = timer.stop()\n",
    "            timings[rep] = elapsed\n",
    "\n",
    "    return np.mean(timings), np.std(timings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1JV1s-ssscv"
   },
   "source": [
    "## Train full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1744637145430,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "FNO_oyLGsmT0",
    "outputId": "e3a9b9cb-b550-4581-ef4f-8a959f8fd3dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already trained. Loading from full_model.pth\n"
     ]
    }
   ],
   "source": [
    "train(full_model, train_loader, epochs=10, save_path=\"full_model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFS44nQpwg9Z"
   },
   "source": [
    "## Evaluate full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 100206,
     "status": "ok",
     "timestamp": 1744636572612,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "j5fVkI49wgPG",
    "outputId": "d73ca6c2-6840-456f-b7df-6d20dea66b07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (full): 79.33%\n",
      "Size (full): 44.78 MB\n",
      "Latency (full, on gpu): 16.22 ± 0.03 ms\n",
      "Latency (full, on cpu): 1088.15 ± 90.12 ms\n"
     ]
    }
   ],
   "source": [
    "# evaluate accuracy\n",
    "accuracy_full = evaluate(full_model, 'cuda')\n",
    "print(f\"Accuracy (full): {accuracy_full*100:.2f}%\")\n",
    "\n",
    "# get model size\n",
    "size_mb_full = os.path.getsize(\"full_model.pth\") / 1e6\n",
    "print(f\"Size (full): {size_mb_full:.2f} MB\")\n",
    "\n",
    "# estimate latency on GPU\n",
    "example_input = torch.rand(128, 3, 32, 32).cuda()\n",
    "full_model.cuda()\n",
    "latency_mu_full_gpu, latency_std_full_gpu = estimate_latency(full_model, example_input)\n",
    "print(f\"Latency (full, on gpu): {latency_mu_full_gpu:.2f} ± {latency_std_full_gpu:.2f} ms\")\n",
    "\n",
    "# estimate latency on CPU\n",
    "example_input = torch.rand(128, 3, 32, 32).cpu()\n",
    "full_model.cpu()\n",
    "latency_mu_full_cpu, latency_std_full_cpu = estimate_latency(full_model, example_input)\n",
    "print(f\"Latency (full, on cpu): {latency_mu_full_cpu:.2f} ± {latency_std_full_cpu:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ_p9LCoujnp"
   },
   "source": [
    "## Apply Quantization for CPU inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8249,
     "status": "ok",
     "timestamp": 1744637188371,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "jzIIkkwItW-2",
    "outputId": "fc8d568c-0419-42e1-d39c-b9e5e5c04806"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/observer.py:244: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# define qconfig and prepare model for quantization\n",
    "full_model.cpu()\n",
    "full_model.eval()\n",
    "qconfig = get_default_qconfig(\"fbgemm\")\n",
    "# qconfig = get_default_qconfig('qnnpack')\n",
    "qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
    "\n",
    "example_inputs = torch.rand(128, 3, 32, 32).cuda() # Example input for calibration\n",
    "# prepared_model = prepare_fx(full_model, {\"\": qconfig}, example_inputs=example_inputs)\n",
    "prepared_model = prepare_fx(full_model, qconfig_mapping, example_inputs=example_inputs)\n",
    "\n",
    "\n",
    "# calibrate model with real data\n",
    "with torch.no_grad():\n",
    "    for images, _ in calibration_loader:\n",
    "        prepared_model(images)\n",
    "        # calibration doesn't need targets, only forward pass\n",
    "\n",
    "# convert to quantized model\n",
    "ptq_model = convert_fx(prepared_model)\n",
    "\n",
    "# save model\n",
    "torch.save(ptq_model.state_dict(), \"ptq_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8FhMY8mwmTk"
   },
   "source": [
    "## Evaluate post-training-quantization (PTQ) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 827
    },
    "executionInfo": {
     "elapsed": 88679,
     "status": "error",
     "timestamp": 1744637498890,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "6vXKh39mwmmu",
    "outputId": "64b87010-0b9f-4489-c888-f4535e112506"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (PTQ): 79.10%\n",
      "Size (PTQ): 11.30 MB\n",
      "Latency (PTQ, on cpu): 695.72 ± 29.66 ms\n"
     ]
    }
   ],
   "source": [
    "# evaluate accuracy\n",
    "ptq_model.cpu()\n",
    "accuracy_ptq = evaluate(ptq_model, 'cpu')\n",
    "print(f\"Accuracy (PTQ): {accuracy_ptq*100:.2f}%\")\n",
    "\n",
    "# get model size\n",
    "size_mb_ptq = os.path.getsize(\"ptq_model.pth\") / 1e6\n",
    "print(f\"Size (PTQ): {size_mb_ptq:.2f} MB\")\n",
    "\n",
    "# estimate latency\n",
    "example_input = torch.rand(128, 3, 32, 32)\n",
    "latency_mu_ptq, latency_std_ptq = estimate_latency(ptq_model, example_input)\n",
    "print(f\"Latency (PTQ, on cpu): {latency_mu_ptq:.2f} ± {latency_std_ptq:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Aware Training (QAT) on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already trained. Loading from full_model.pth\n"
     ]
    }
   ],
   "source": [
    "# reload full model\n",
    "full_model_qat = get_resnet18_for_cifar10()\n",
    "train(full_model_qat, train_loader, epochs=10, save_path=\"full_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.0052\n",
      "Training complete. Model saved to qat_model.pth\n"
     ]
    }
   ],
   "source": [
    "from torch.quantization import prepare_qat, convert, get_default_qconfig\n",
    "\n",
    "# backend = \"fbgemm\"  # running on a x86 CPU. Use \"qnnpack\" if running on ARM.\n",
    "backend = \"qnnpack\"\n",
    "# qconfig = get_default_qconfig('qnnpack')\n",
    "\n",
    "# torch.backends.quantized.engine = 'fbgemm'\n",
    "\n",
    "# full_model_qat.eval()\n",
    "# # Fuse the model in place rather manually.\n",
    "# full_model_qat = torch.quantization.fuse_modules(full_model_qat, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
    "# for module_name, module in full_model_qat.named_children():\n",
    "#     if \"layer\" in module_name:\n",
    "#         for basic_block_name, basic_block in module.named_children():\n",
    "#             torch.quantization.fuse_modules(basic_block, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\"]], inplace=True)\n",
    "#             for sub_block_name, sub_block in basic_block.named_children():\n",
    "#                 if sub_block_name == \"downsample\":\n",
    "#                     torch.quantization.fuse_modules(sub_block, [[\"0\", \"1\"]], inplace=True)\n",
    "\n",
    "# class QuantizedResNet18(nn.Module):\n",
    "#     def __init__(self, model_fp32):\n",
    "#         super(QuantizedResNet18, self).__init__()\n",
    "#         # QuantStub converts tensors from floating point to quantized.\n",
    "#         # This will only be used for inputs.\n",
    "#         self.quant = torch.quantization.QuantStub()\n",
    "#         # DeQuantStub converts tensors from quantized to floating point.\n",
    "#         # This will only be used for outputs.\n",
    "#         self.dequant = torch.quantization.DeQuantStub()\n",
    "#         # FP32 model\n",
    "#         self.model_fp32 = model_fp32\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # manually specify where tensors will be converted from floating\n",
    "#         # point to quantized in the quantized model\n",
    "#         x = self.quant(x)\n",
    "#         x = self.model_fp32(x)\n",
    "#         # manually specify where tensors will be converted from quantized\n",
    "#         # to floating point in the quantized model\n",
    "#         x = self.dequant(x)\n",
    "#         return x\n",
    "\n",
    "# quantized_model = QuantizedResNet18(model_fp32=full_model_qat)\n",
    "\n",
    "\"\"\"Insert stubs\"\"\"\n",
    "full_model_qat = nn.Sequential(torch.quantization.QuantStub(), \n",
    "                  full_model_qat, \n",
    "                  torch.quantization.DeQuantStub())\n",
    "\n",
    "\n",
    "#adapt model\n",
    "# full_model_qat.qconfig = get_default_qconfig(\"fbgemm\")\n",
    "# full_model_qat.qconfig = get_default_qconfig(\"qnnpack\")\n",
    "full_model_qat.train()\n",
    "full_model_qat.qconfig = get_default_qconfig(backend)\n",
    "\n",
    "\n",
    "# from torch.quantization import QConfig, default_observer, default_fake_quant, default_weight_fake_quant\n",
    "\n",
    "# qat_qconfig_per_tensor = QConfig(\n",
    "#     activation=default_fake_quant,\n",
    "#     weight=default_weight_fake_quant \n",
    "# )\n",
    "\n",
    "# full_model_qat.qconfig = qat_qconfig_per_tensor\n",
    "prepare_qat(full_model_qat, inplace=True)\n",
    "\n",
    "# consider changing to 3\n",
    "train(full_model_qat, train_loader, epochs=1, save_path=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to quantized model\n",
    "# full_model_qat.cpu()\n",
    "full_model_qat.eval()\n",
    "qat_model = convert(full_model_qat, inplace=True)\n",
    "# qat_model = convert(full_model_qat, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "getCudnnDataTypeFromScalarType() not supported for QUInt8",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# evaluate accuracy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m accuracy_qat = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqat_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAccuracy (QAT): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_qat*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(model, device_str)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_str:\n\u001b[32m     10\u001b[39m     x, y = x.to(device), y.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m preds = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m.argmax(\u001b[32m1\u001b[39m)\n\u001b[32m     12\u001b[39m correct += (preds == y).sum().item()\n\u001b[32m     13\u001b[39m total += y.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py:285\u001b[39m, in \u001b[36mResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py:268\u001b[39m, in \u001b[36mResNet._forward_impl\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    269\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.bn1(x)\n\u001b[32m    270\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.relu(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/ao/nn/quantized/modules/conv.py:594\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    590\u001b[39m     _reversed_padding_repeated_twice = _reverse_repeat_padding(\u001b[38;5;28mself\u001b[39m.padding)\n\u001b[32m    591\u001b[39m     \u001b[38;5;28minput\u001b[39m = F.pad(\n\u001b[32m    592\u001b[39m         \u001b[38;5;28minput\u001b[39m, _reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m    593\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantized\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_packed_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mzero_point\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py:1158\u001b[39m, in \u001b[36mOpOverloadPacket.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[32m   1157\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: getCudnnDataTypeFromScalarType() not supported for QUInt8"
     ]
    }
   ],
   "source": [
    "# evaluate accuracy\n",
    "accuracy_qat = evaluate(qat_model, 'cpu')\n",
    "print(f\"Accuracy (QAT): {accuracy_qat*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to quantized model\n",
    "full_model_qat.eval()\n",
    "qat_model = convert(full_model_qat, inplace=False)\n",
    "\n",
    "# evaluate accuracy\n",
    "accuracy_qat = evaluate(qat_model, '')\n",
    "print(f\"Accuracy (QAT): {accuracy_qat*100:.2f}%\")\n",
    "\n",
    "# get model size\n",
    "size_mb_qat = os.path.getsize(\"qat_model.pth\") / 1e6\n",
    "print(f\"Size (QAT): {size_mb_qat:.2f} MB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is quantized:\", isinstance(qat_model.conv1, torch.nn.quantized.Conv2d))\n",
    "\n",
    "x = torch.randn(1, 3, 224, 224).float()\n",
    "print(\"Input dtype:\", x.dtype, \"device:\", x.device)\n",
    "\n",
    "torch.backends.quantized.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qat_model)\n",
    "print(qat_model.conv1)  # or whatever your first conv layer is\n",
    "\n",
    "x = torch.randn(1, 3, 224, 224)  # assuming input shape for ResNet\n",
    "out = qat_model(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate latency\n",
    "example_input = torch.rand(128, 3, 32, 32)\n",
    "latency_mu_qat, latency_std_qat = estimate_latency(qat_model, example_input)\n",
    "print(f\"Latency (QAT, on cpu): {latency_mu_qat:.2f} ± {latency_std_qat:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaCIiXtoFbFF"
   },
   "source": [
    "# Quantize using GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup required packages for GPU Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch_tensorrt\n",
    "!pip install nvidia-modelopt[all]\n",
    "!pip install onnx\n",
    "!pip install onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required imports for GPU Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_tensorrt\n",
    "import modelopt.torch.quantization as mtq\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1744639130324,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "vFvnimsTiAs6",
    "outputId": "15ae4d66-8462-4ff2-c0b0-66c25b42ea2f"
   },
   "outputs": [],
   "source": [
    "# reload full model\n",
    "full_model = get_resnet18_for_cifar10()\n",
    "train(full_model, train_loader, epochs=10, save_path=\"full_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1744639131506,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "NMNB7UR_Gp4Y"
   },
   "outputs": [],
   "source": [
    "# define calibration loop\n",
    "def calibration_loop(model):\n",
    "    for batch, _ in calibration_loader:\n",
    "        model(batch.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 510,
     "status": "ok",
     "timestamp": 1744639133391,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "0YJBSc9YG24s",
    "outputId": "792de4a3-fd92-485e-d495-fe3f7137f468"
   },
   "outputs": [],
   "source": [
    "# Select quantization config\n",
    "config = mtq.INT8_SMOOTHQUANT_CFG\n",
    "\n",
    "# Quantize the model and perform calibration (PTQ)\n",
    "ptq_model_gpu = mtq.quantize(full_model, config, calibration_loop)\n",
    "\n",
    "# save model\n",
    "torch.save(ptq_model_gpu.state_dict(), \"ptq_model_gpu.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 108,
     "status": "ok",
     "timestamp": 1744639136460,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "GRV_9OjKKWih"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(ptq_model_gpu.state_dict(), \"ptq_model_gpu.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6978,
     "status": "ok",
     "timestamp": 1744639145314,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "VuA-LdqZKYie",
    "outputId": "92348a08-4da7-4921-9083-7d5db97cdac1"
   },
   "outputs": [],
   "source": [
    "# evaluate accuracy\n",
    "accuracy_ptq_gpu = evaluate(ptq_model_gpu, 'cuda')\n",
    "print(f\"Accuracy (PTQ): {accuracy_ptq_gpu*100:.2f}%\")\n",
    "\n",
    "# get model size\n",
    "size_mb_ptq_gpu = os.path.getsize(\"ptq_model_gpu.pth\") / 1e6\n",
    "print(f\"Size (PTQ): {size_mb_ptq_gpu:.2f} MB\")\n",
    "\n",
    "# estimate latency\n",
    "example_input = torch.rand(128, 3, 32, 32).to(device)\n",
    "latency_mu_ptq_gpu, latency_std_ptq_gpu = estimate_latency(ptq_model_gpu, example_input)\n",
    "print(f\"Latency (PTQ, on gpu): {latency_mu_ptq_gpu:.2f} ± {latency_std_ptq_gpu:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1966,
     "status": "ok",
     "timestamp": 1744639157290,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "9niknt2pjvOe",
    "outputId": "686a24e4-281b-407d-a9d1-7e868be8f1bd"
   },
   "outputs": [],
   "source": [
    "# export to onnx\n",
    "example_input = torch.rand(128, 3, 32, 32).cuda()\n",
    "torch.onnx.export(ptq_model_gpu, example_input, \"ptq_model_gpu.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1744639162791,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "60_K3a3Hl3qi"
   },
   "outputs": [],
   "source": [
    "# load onnx model\n",
    "onnx_model = onnx.load(\"ptq_model_gpu.onnx\")\n",
    "\n",
    "# check that the model is well formed\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 172,
     "status": "ok",
     "timestamp": 1744639578680,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "noh2MnUJnzDB",
    "outputId": "b1abe841-e94a-4c1f-8be6-206de379e608"
   },
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "session = ort.InferenceSession(\n",
    "    \"ptq_model_gpu.onnx\",\n",
    "    providers=[\"CUDAExecutionProvider\"]  # 👈 use GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1744639417791,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "zEzCDkn8lvIx",
    "outputId": "402fc86e-a0b3-46ed-d2bc-e83bc8cc52f6"
   },
   "outputs": [],
   "source": [
    "# # evaluate accuracy\n",
    "# accuracy_ptq_gpu_onnx = evaluate(onnx_model, 'cuda')\n",
    "# print(f\"Accuracy (PTQ): {accuracy_ptq_gpu_onnx*100:.2f}%\")\n",
    "\n",
    "# get model size\n",
    "size_mb_ptq_gpu_onnx = os.path.getsize(\"ptq_model_gpu.onnx\") / 1e6\n",
    "print(f\"Size (PTQ): {size_mb_ptq_gpu_onnx:.2f} MB\")\n",
    "\n",
    "# # estimate latency\n",
    "# example_input = torch.rand(128, 3, 32, 32).to(device)\n",
    "# latency_mu_ptq_gpu_onnx, latency_std_ptq_gpu_onnx = estimate_latency(onnx_model, example_input)\n",
    "# print(f\"Latency (PTQ, on gpu): {latency_mu_ptq_gpu_onnx:.2f} ± {latency_std_ptq_gpu_onnx:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "error",
     "timestamp": 1744638846701,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "FPmHu1qljvDe",
    "outputId": "7a6cd8f2-cd55-4837-e61c-c2b21074647f"
   },
   "outputs": [],
   "source": [
    "# Compress the model\n",
    "mtq.compress(ptq_model_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "executionInfo": {
     "elapsed": 1918,
     "status": "error",
     "timestamp": 1744634296124,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "W2xT0LL1KYdz",
    "outputId": "cce13d35-5a4d-4120-9780-7344810d3b3c"
   },
   "outputs": [],
   "source": [
    "from modelopt.torch.quantization.utils import export_torch_mode\n",
    "import torch_tensorrt as torchtrt\n",
    "\n",
    "example_input = torch.rand(128, 3, 32, 32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with export_torch_mode():\n",
    "        # Compile the model with Torch-TensorRT Dynamo backend\n",
    "\n",
    "        exp_program = torch.export.export(ptq_model, (example_input,), strict=False)\n",
    "        enabled_precisions = {torch.int8}\n",
    "        # enabled_precisions = {torch.float8_e4m3fn}\n",
    "        trt_model = torchtrt.dynamo.compile(\n",
    "            exp_program,\n",
    "            inputs=[example_input],\n",
    "            enabled_precisions=enabled_precisions,\n",
    "            min_block_size=1,\n",
    "            debug=False,\n",
    "        )\n",
    "        # You can also use torch compile path to compile the model with Torch-TensorRT:\n",
    "        # trt_model = torch.compile(model, backend=\"tensorrt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UZP-kxBKYYq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hDFinDtsmPA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdtUV6rGsJYE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6397,
     "status": "ok",
     "timestamp": 1744590564621,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "Q_TWvYfLrhES",
    "outputId": "b9c049bf-8434-4b38-a4eb-c0905ea29cec"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tuqd6gNZrukG"
   },
   "source": [
    "## Load and adapt MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 503,
     "status": "ok",
     "timestamp": 1744590565121,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "gVX7WXThrysH",
    "outputId": "70db76b4-843c-4a4b-ef45-471188a2da7b"
   },
   "outputs": [],
   "source": [
    "model_fp32 = models.mobilenet_v2(pretrained=True)\n",
    "model_fp32.classifier[1] = nn.Linear(model_fp32.last_channel, 10)\n",
    "model_fp32.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyfNZ2V1sDs9"
   },
   "source": [
    "## Apply dynamic quantization to Linear layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDlkW0BIsFaL"
   },
   "outputs": [],
   "source": [
    "model_int8 = quantize_dynamic(model_fp32, {nn.Linear}, dtype=torch.qint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4aPogLPsIXr"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 839522,
     "status": "ok",
     "timestamp": 1744591404677,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "wJXY1MbrsKD_",
    "outputId": "0bf54816-6807-4809-d9fa-89ed4c6ac4c3"
   },
   "outputs": [],
   "source": [
    "# Evaluate function\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Run evaluation\n",
    "acc_original = evaluate(model_fp32, test_loader)\n",
    "print(f\"Original Model Accuracy: {acc_original*100:.2f}%\")\n",
    "\n",
    "acc = evaluate(model_int8, test_loader)\n",
    "print(f\"PTQ Quantized Model Accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 868072,
     "status": "ok",
     "timestamp": 1744592806159,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "-QHv2JmByFpg",
    "outputId": "bc9fac16-84e7-45f6-e1a0-dd529b04562a"
   },
   "outputs": [],
   "source": [
    "acc_original = evaluate(model_fp32, test_loader)\n",
    "print(f\"Original Model Accuracy: {acc_original:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBrhyef9sM66"
   },
   "source": [
    "## Measure inference time and model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6678,
     "status": "ok",
     "timestamp": 1744593224998,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "K3khBiH4rGr0",
    "outputId": "91ca5b70-1aa3-4f68-a7bb-0bd8ac7946a1"
   },
   "outputs": [],
   "source": [
    "def measure_latency(model, device, input_shape=(1, 3, 224, 224), runs=100):\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(input_shape).to(device)\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        for _ in range(runs):\n",
    "            model(dummy_input)\n",
    "        end = time.time()\n",
    "    return (end - start) / runs\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "latency_org = measure_latency(model_fp32, device)\n",
    "print(f\"Average inference latency (org): {latency_org * 1000:.2f} ms\")\n",
    "\n",
    "latency = measure_latency(model_int8, device)\n",
    "print(f\"Average inference latency (PTQ): {latency * 1000:.2f} ms\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvB9Jg19smi3"
   },
   "source": [
    "## Save and report model size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1744593060779,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "Q4u5y7cMsqCX",
    "outputId": "a3e59004-bb0c-42c1-8d83-c7f10c46bef3"
   },
   "outputs": [],
   "source": [
    "# Save and report model size\n",
    "torch.save(model_int8.state_dict(), \"ptq_model.pth\")\n",
    "size_mb = os.path.getsize(\"ptq_model.pth\") / 1e6\n",
    "print(f\"Model size (PTQ): {size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1744593107177,
     "user": {
      "displayName": "Arik Poznanski",
      "userId": "17215250503704805691"
     },
     "user_tz": -180
    },
    "id": "HyGVMQ_32YJr",
    "outputId": "a3fa49ac-7c41-46c9-c59e-5667a06e4f41"
   },
   "outputs": [],
   "source": [
    "torch.save(model_fp32.state_dict(), \"org_model.pth\")\n",
    "size_mb = os.path.getsize(\"org_model.pth\") / 1e6\n",
    "print(f\"Model size (org): {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdscHNOl2jfs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOJ7eNLZPT5dGBuCMUjogIL",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
